{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0XNGOH8Rtdf",
        "colab_type": "code",
        "outputId": "5f7352f1-c955-4bff-f064-afe48e2d7878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "!pip install scipy==1.2.0\n",
        "!pip install pmdarima\n",
        "!pip install tbats\n",
        "!pip install python-geohash\n",
        "!pip install catboost \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = \"drive/My Drive/training.csv\"\n",
        "dataset = pd.read_csv(path)\n",
        "# read from file option\n",
        "#dataset = pd.read_csv('/content/training.csv')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.16.4)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.21.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.12.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.16.4)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.29.10)\n",
            "Requirement already satisfied: scipy<1.3,>=1.2 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->pmdarima) (0.5.1)\n",
            "Requirement already satisfied: tbats in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tbats) (1.16.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from tbats) (0.0)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.6/dist-packages (from tbats) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tbats) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->tbats) (0.21.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.24.2)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.29.10)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima->tbats) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima->tbats) (2018.9)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->pmdarima->tbats) (0.5.1)\n",
            "Requirement already satisfied: python-geohash in /usr/local/lib/python3.6/dist-packages (0.8.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujsPMXbRxmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# ignore all future warnings \n",
        "#simplefilter(action='ignore', category=FutureWarning) #hide https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore') #https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
        "\n",
        "from datetime import timedelta  \n",
        "from datetime import datetime\n",
        "import time\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from math import sqrt\n",
        "# mapping\n",
        "import folium\n",
        "from folium import plugins\n",
        "import geohash\n",
        "\n",
        "# models\n",
        "from statsmodels.tsa.arima_model import ARIMA # tried a bit but not sure of parameters to use\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from pmdarima import auto_arima\n",
        "from tbats import TBATS\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from fbprophet import Prophet\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# functions tried to use\n",
        "def getNaive(groups, key_tuple, default):\n",
        "  try:\n",
        "    return groups.get_group(key_tuple).mean().demand\n",
        "  except:\n",
        "    print('returning default as unable to find ', key_tuple)\n",
        "    return default\n",
        "\n",
        "def adjMean(testData, currPred):\n",
        "  finalPred=currPred\n",
        "  meanAdj = [-0.000401695755597994,\t-0.001327695755598,\t0.004418304244402,\t0.00649630424440201,\t0.011890304244402,\t-0.005788695755598,\t-0.018775695755598]\n",
        "  for row in testData.itertuples():\n",
        "    finalPred[i]=currPred[i]+meanAdj[getattr(row,'dayOfWeek')]\n",
        "  return finalPred\n",
        "\n",
        "# typo \n",
        "def rsme(y_actual, y_predicted,model):\n",
        "  result = sqrt(mean_squared_error(y_actual, y_predicted))\n",
        "  #print(round(result,5), ' rsme for model ', model) # left-align for easier seeing\n",
        "  return result\n",
        "\n",
        "def avgRMSE(y_predicted, testDatas, model):\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash],testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing\n",
        "  #return mean(rsmes)\n",
        "\n",
        "# just to see if best estimator is skew\n",
        "def adjRMSE(y_predicted, testDatas, model):\n",
        "  # adj demand up\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash]+0.005,testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing\n",
        "  \n",
        "  # adj demand down\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash]-0.005,testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing  \n",
        "  #return mean(rsmes)\n",
        "\n",
        "def RSME(y_predicted, testDatas, model):\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash],testDatas[geohash].demand, model))\n",
        "  print(round(mean(rsmes),5), ' rsme for model ', model) # left-align for easier seeing\n",
        "  #return mean(rsmes)\n",
        "  \n",
        "# handle less than expected data (test Data <5 or trainData empty)\n",
        "def predictUnexpectedData(randGeoHash, trainData, testData):\n",
        "  if(len(trainData) == 0):\n",
        "    print('no train data, just predict based on average of 0.105090695755598')\n",
        "    return np.repeat(0.105090695755598,len(testData))\n",
        "  else:   \n",
        "    print('not enough train data, just predict based last train data')\n",
        "    return np.repeat(trainData[-1:].demand.values,len(testData))\n",
        "  \n",
        "# Update only if invalid\n",
        "def predictInvalidPreds(predictions, geohash):\n",
        "  for i in range(len(predictions)):\n",
        "    pred = predictions[i]\n",
        "    if np.isfinite(pred) == False:\n",
        "      print('updating invalid prediction of ', pred, ' for geohash ', geohash, ' to avg of 0.105090695755598')\n",
        "      predictions[i] = 0.105090695755598\n",
        "    elif pred < 0:\n",
        "      print('updating negative prediction of ', pred, ' for geohash ', geohash, ' to 0')\n",
        "      predictions[i] = 0\n",
        "    elif pred > 1:\n",
        "      print('updating above 1 prediction of ', pred, ' for geohash ', geohash, ' to 1')\n",
        "      predictions[i] = 1\n",
        "  return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ioBce94ckVQ",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3SSpmQAPMbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# created 'x' for easier sorting. datetimeindex don't seem easy to use\n",
        "timestamp_dict = {'0:0':0,\t'0:15':1,\t'0:30':2,\t'0:45':3,\t'1:0':4,\t'1:15':5,\t'1:30':6,\t'1:45':7,\t'2:0':8,\t'2:15':9,\t'2:30':10,\t'2:45':11,\t'3:0':12,\t'3:15':13,\t'3:30':14,\t'3:45':15,\t'4:0':16,\t'4:15':17,\t'4:30':18,\t'4:45':19,\t'5:0':20,\t'5:15':21,\t'5:30':22,\t'5:45':23,\t'6:0':24,\t'6:15':25,\t'6:30':26,\t'6:45':27,\t'7:0':28,\t'7:15':29,\t'7:30':30,\t'7:45':31,\t'8:0':32,\t'8:15':33,\t'8:30':34,\t'8:45':35,\t'9:0':36,\t'9:15':37,\t'9:30':38,\t'9:45':39,\t'10:0':40,\t'10:15':41,\t'10:30':42,\t'10:45':43,\t'11:0':44,\t'11:15':45,\t'11:30':46,\t'11:45':47,\t'12:0':48,\t'12:15':49,\t'12:30':50,\t'12:45':51,\t'13:0':52,\t'13:15':53,\t'13:30':54,\t'13:45':55,\t'14:0':56,\t'14:15':57,\t'14:30':58,\t'14:45':59,\t'15:0':60,\t'15:15':61,\t'15:30':62,\t'15:45':63,\t'16:0':64,\t'16:15':65,\t'16:30':66,\t'16:45':67,\t'17:0':68,\t'17:15':69,\t'17:30':70,\t'17:45':71,\t'18:0':72,\t'18:15':73,\t'18:30':74,\t'18:45':75,\t'19:0':76,\t'19:15':77,\t'19:30':78,\t'19:45':79,\t'20:0':80,\t'20:15':81,\t'20:30':82,\t'20:45':83,\t'21:0':84,\t'21:15':85,\t'21:30':86,\t'21:45':87,\t'22:0':88,\t'22:15':89,\t'22:30':90,\t'22:45':91,\t'23:0':92,\t'23:15':93,\t'23:30':94,\t'23:45':95,}\n",
        "try:\n",
        "  dataset['timestampX']=dataset['timestamp']\n",
        "  dataset.timestampX=dataset.timestampX.map(timestamp_dict) #replace is so slow\n",
        "  dataset['dayOfWeek']=dataset['day']%7\n",
        "except:\n",
        "  print('unexpected error during copy')\n",
        " \n",
        "dataset['x']=(dataset.day-1)*96 + dataset.timestampX\n",
        "#origData = dataset.copy()\n",
        "dataset['datetime']=pd.to_timedelta(dataset.loc[:,'day'].astype(str) + ' days ' + dataset.loc[:,'timestamp'] + \":00\")\n",
        "dataset.datetime=pd.to_datetime(dataset.loc[:,'datetime'])\n",
        "\n",
        "dataset=dataset.sort_values(by=['geohash6', 'x'])\n",
        "#dataset.head()\n",
        "\n",
        "# just store last sequential demand\n",
        "# TODO should not store demand of prev since unknown? \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUMKeaKWVaoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def splitData1(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1) # store last demand \n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1) # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand[1:] #remove first row since NA  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)[1:]\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# only diff from 1 is to drop row when don't have prev interval\n",
        "def splitData2(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1)[1:] # store last demand and remove first row since NA\n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1)[1:] # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainData=trainData[trainData.prevX==trainData.x-1] #don't do on test data else only accept those with sequential (though final test should be)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1) \n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# only diff from 2 is to drop prevX too\n",
        "def splitData3(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1)[1:] # store last demand and remove first row since NA\n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1)[1:] # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainData=trainData[trainData.prevX==trainData.x-1] #don't do on test data else only accept those with sequential (though final test should be)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','prevX','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','prevX','demand'], axis=1) \n",
        "  return trainData,trainLabel,testData\n",
        "# use neighbor \n",
        "def splitData4(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1) # store last demand \n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1) # for checking if got prev interval\n",
        "  \n",
        "  neighborDemand=dataset[dataset['geohash6'].isin(geohash.neighbors(randGeoHash)) & dataset['x'].isin(hashDataset1.x)].groupby('x').mean()\n",
        "  neighborDemand['neighborPrevDemand']=neighborDemand.demand.shift(1)\n",
        "  hashDataset1=pd.merge(hashDataset1, neighborDemand[['neighborPrevDemand']], on='x')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  # remove first row since NA\n",
        "  trainLabel=trainData.demand[1:]  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)[1:]\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data\n",
        "def splitData5(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand','dayOfWeek'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand','dayOfWeek'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data with zero for empty geohash\n",
        "def splitData6(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # for consistency, testData is still the last 5 with demand  \n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  missingRows=pd.DataFrame() \n",
        "  missingRows['x']=missing_elements(trainData.x.values)\n",
        "  missingRows['demand']=0    \n",
        "  missingRows['day']=np.floor(missingRows['x']/96)+1\n",
        "  missingRows['dayOfWeek']=missingRows['day']%7\n",
        "  missingRows['timestampX']=np.floor(missingRows['x']%96)\n",
        "  trainData=trainData6.append(missingRows,sort=False)   \n",
        "  trainData=trainData6.sort_values('x')  \n",
        "    \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data with median field\n",
        "def splitData7(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  newFrame=pd.DataFrame()\n",
        "  newFrame['dayOfWeek']=range(0,6)\n",
        "  newFrame['dayOfWeekMedian']=dataset.groupby(['dayOfWeek']).demand.median()\n",
        "  hashDataset1=pd.merge(hashDataset1,newFrame,on='dayOfWeek')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def splitData8(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  newFrame=pd.DataFrame()\n",
        "  newFrame['dayOfWeek']=range(0,6)\n",
        "  newFrame['dayOfWeekMedian']=dataset.groupby(['dayOfWeek']).demand.median()\n",
        "  hashDataset1=pd.merge(hashDataset1,newFrame,on='dayOfWeek')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand','prevDemand','prevX'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand','prevDemand','prevX'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def splitData9(hashDataset, allSet):  \n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # prev neighbor data within 14 days\n",
        "  neighborDemand=dataset[(dataset.day>=testData.iloc[0].day) & (dataset.x<testData.iloc[0].x) & dataset['geohash6'].isin(geohash.neighbors(randGeoHash))].groupby(['geohash6'])\n",
        "  neighborDemandMean=neighborDemand.demand.mean().values\n",
        "  neighborDemandMax=neighborDemand.demand.max().values\n",
        "  for i in range(8):\n",
        "    try:\n",
        "      hashDataset1['nMean'+str(i)]=neighborDemandMean[i]\n",
        "      hashDataset1['nMax'+str(i)]=neighborDemandMax[i]\n",
        "    except IndexError:\n",
        "      hashDataset1['nMax'+str(i)]=hashDataset1['nMean'+str(i)]=0\n",
        "\n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# create count of zeros field by timestamp\n",
        "def splitData10(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # for consistency, testData is still the last 5 with demand  \n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  if(len(trainData)>1):\n",
        "    missingRows=pd.DataFrame()   \n",
        "    mElements=missing_elements(trainData.x.values)\n",
        "    missingRows['x']=mElements\n",
        "    missingRows['demand']=0    \n",
        "    missingRows['day']=np.floor(missingRows['x']/96)+1\n",
        "    missingRows['dayOfWeek']=missingRows['day']%7\n",
        "    missingRows['timestampX']=np.floor(missingRows['x']%96)\n",
        "    trainData=trainData.append(missingRows,sort=False)\n",
        "    trainData=trainData.sort_values('x')  \n",
        "\n",
        "  emptyTime=trainData[trainData.demand==0].groupby('timestampX').demand.count()\n",
        "  trainData=trainData.join(emptyTime,on='timestampX',rsuffix='EmptyTimeCount')\n",
        "  testData=testData.join(emptyTime,on='timestampX',rsuffix='EmptyTimeCount')\n",
        "  trainData.demandEmptyTimeCount.fillna(0, inplace=True)\n",
        "  testData.demandEmptyTimeCount.fillna(0, inplace=True)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)  \n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def missing_elements(L):\n",
        "    start, end = L[0], L[-1]\n",
        "    return sorted(set(range(start, end + 1)).difference(L))\n",
        "\n",
        "# for rnn\n",
        "def create_RNNset(trainData,trainLabel,testData):\n",
        "  trainX = np.array(trainData.x).reshape(len(trainData),1,-1)\n",
        "  trainY = trainLabel\n",
        "  testX =  np.array(testData.x).reshape(len(testData),1,-1)\n",
        "  \n",
        "  return trainX,trainY,testX\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvAyJD2MBSgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed4c19cc-0bc2-40a7-d2d4-4b701ca1c1ce"
      },
      "source": [
        "# for debugging\n",
        "len(dataset.geohash6.unique())"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjZryQ4V45H-",
        "colab_type": "text"
      },
      "source": [
        "# Core Model Running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6e0LMm8s5k4",
        "colab_type": "code",
        "outputId": "5a224c7d-7327-4c08-f222-64dc4b55ac93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 29705
        }
      },
      "source": [
        "# TODO randomise the start day and timestamp to do prediction for each geohash prior to running models\n",
        "# assume sorted from small to big already\n",
        "randGeoHashes=dataset.geohash6.unique()\n",
        "#randGeoHashes=np.random.choice(dataset.geohash6.unique(),30)\n",
        "#randGeoHashes=['qp02yu','qp02yv','qp03zx', 'qp08fu', 'qp090p','qp03q4', 'qp092d','qp09jy','qp09dz','qp03qp']\n",
        "\n",
        "# stores preds as dict for easier \n",
        "preds={}\n",
        "tests=['predictedTSDOW',\n",
        "'predictedTS',\n",
        "'predictedDOW',\n",
        "'predictedArima1',\n",
        "'predictedArima2',\n",
        "'predictedArima3',\n",
        "'predictedXGBoost1',\n",
        "'predictedXGBoost2',\n",
        "'predictedXGBoost3',\n",
        "'predictedXGBoost4',\n",
        "'predictedXGBoost5-perfect',\n",
        "'predictedXGBoost6',       \n",
        "'predictedXGBoost7-testData4',\n",
        "'predictedXGBoost8',       \n",
        "'predictedXGBoost9',       \n",
        "'predictedTBATS',\n",
        "'predictedRandForestRegressor1',\n",
        "'predictedRandForestRegressor2',\n",
        "'predictedRandForestRegressor3',\n",
        "'predictedRandForestRegressor4-perfect',\n",
        "'predictedRandForestRegressor5',\n",
        "'predictedRandForestRegressor6',\n",
        "'predictedRandForestRegressor7',\n",
        "'predictedRandForestRegressor8',       \n",
        "'predictedRandForestRegressor9',       \n",
        "'gradientBoostRegressor1',\n",
        "'gradientBoostRegressor2',\n",
        "'gradientBoostRegressor3',\n",
        "'gradientBoostRegressor4',\n",
        "'gradientBoostRegressor5',       \n",
        "'gradientBoostRegressor6',\n",
        "'gradientBoostRegressor7',  \n",
        "'catBoost1',    \n",
        "'ensembleXGBoost2Gradient4',\n",
        "'logReg',\n",
        "'gaussian',       \n",
        "'lgm1',\n",
        "'SGDR',\n",
        "'keras1',\n",
        "'keras2',\n",
        "'keras3-notdone',\n",
        "'fb-prophet',\n",
        "'fb-prophet2'\n",
        "]\n",
        "for pred in tests:\n",
        "  preds[pred]={}\n",
        "\n",
        "# this is to know which tests was really used\n",
        "workingHashForPrinting=''\n",
        "testDatas = {}\n",
        "\n",
        "# idea as keras is so slow to train and more epoch usually better. just retrain one over time instead of new 1 per geohash\n",
        "#keras2 = Sequential()\n",
        "#keras2.add(LSTM(4, input_shape=(1, 1)))\n",
        "#keras2.add(Dense(1))\n",
        "#keras2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# idea 2 is to train keras over whole data then just predict\n",
        "#keras3 = Sequential()\n",
        "#keras3.add(LSTM(4, input_shape=(1, 1)))\n",
        "#keras3.add(Dense(1))\n",
        "#keras3.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# reuse and just keep fitting\n",
        "gbrt3=GradientBoostingRegressor(n_estimators=100)\n",
        "reg8 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "xg_reg8 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True)\n",
        "    \n",
        "# variables for model in following sections\n",
        "# naive, auto-arima, xgboost + GBR, tbats, randforestreg, rnn, FB prophet\n",
        "boolSectionsToRun = [False,False,True,False,False,False,False]\n",
        "showGraph=False\n",
        "showTimeTakenForEach=False\n",
        "for randGeoHash in randGeoHashes:\n",
        "  print('geohash is ',randGeoHash)\n",
        "  \n",
        "  #print('neighbours are ',geohash.neighbors(randGeoHash))\n",
        "  randDataSet=dataset.loc[dataset.geohash6==randGeoHash]\n",
        "  # abnormal data\n",
        "  # qp02yu had only 2 points so cannot really predict since assume T+1 to T+5\n",
        "  # qp02yv if choose 5 test data (start from day 24), there is only 1 training data within 14 days\n",
        "  # TODO qp03zx, qp08fu auto arima return nan. not sure why\n",
        "  # qp09jy got negative indices \n",
        "  # Could not successfully fit ARIMA to input data. It is likely your data is non-stationary. Please induce stationarity or try a different range of model order params. If your data is seasonal, check the period (m) of the data.\n",
        "  \n",
        "  # cap at 14 days as per requirement\n",
        "  # since data has shortfall for some, take from backwards  \n",
        "  testData=randDataSet[-5:]\n",
        "  trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]\n",
        "  testDatas[randGeoHash] = randDataSet[-5:]\n",
        "  \n",
        "  # sklearn modules prefer 2d array\n",
        "  if(len(trainData)) > 0:\n",
        "    trainDataX = np.array(trainData.x).reshape(len(trainData),-1)\n",
        "  testDataX = np.array(testData.x).reshape(len(testData),-1)     \n",
        "  \n",
        "  # for xgboost, randomforest  \n",
        "  trainData1,trainLabel1,testData1=splitData1(randDataSet, dataset)      \n",
        "  trainData2,trainLabel2,testData2=splitData2(randDataSet, dataset)\n",
        "  trainData3,trainLabel3,testData3=splitData3(randDataSet, dataset)  \n",
        "  #trainData4,trainLabel4,testData4=splitData4(randDataSet, dataset)\n",
        "  trainData5,trainLabel5,testData5=splitData5(randDataSet, dataset)\n",
        "  # don't use as didn't catch exception for missing_elements in rare case\n",
        "  #trainData6,trainLabel6,testData6=splitData6(randDataSet, dataset)\n",
        "  #trainData7,trainLabel7,testData7=splitData7(randDataSet, dataset)\n",
        "  trainData9,trainLabel9,testData9=splitData9(randDataSet, dataset)\n",
        "  trainData10,trainLabel10,testData10=splitData10(randDataSet, dataset)\n",
        "  trainDataForTest = trainData.drop(['geohash6','timestamp','datetime', 'day', 'dayOfWeek', 'x', 'timestampX'], axis=1) \n",
        "  testDataForTest = testData.drop(['geohash6','timestamp','datetime','day', 'dayOfWeek', 'x', 'timestampX'], axis=1) \n",
        "  \n",
        "  # for robustness, predict unexpected data set (too few training or testing)\n",
        "  if(len(randDataSet) <= 5 or len(trainData) <= 1 or len(trainData1) <= 1  or len(trainData2) <= 1 ):\n",
        "    result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    for pred in tests:\n",
        "      preds[pred][randGeoHash]=result\n",
        "    continue\n",
        "  \n",
        "  if(showGraph):\n",
        "    plt.title('train data for geohash ' + randGeoHash)    \n",
        "    ax1=trainData.plot(x='x', y='demand')    \n",
        "    testData.plot(ax=ax1, x='x', y='demand') #, ax=ax1 if want to see in same\n",
        "\n",
        "  workingHashForPrinting=randGeoHash\n",
        "  for pred in tests:\n",
        "    preds[pred][randGeoHash] = []  \n",
        "  start = time.clock()   \n",
        "  \n",
        "  # Section 0 - naive self-coded with default demand\n",
        "  # super naive prediction based on historical day and timestamp \n",
        "  # 14-days of data too sparse that not much diff when tried median\n",
        "  if(boolSectionsToRun[0]):\n",
        "    meanTrainDataBy_TSDOW=trainData.groupby(['timestamp','dayOfWeek'])\n",
        "    meanTrainDataBy_TS=trainData.groupby(['timestamp'])\n",
        "    meanTrainDataBy_DOW=trainData.groupby(['dayOfWeek'])\n",
        "    defaultDemand=0.10509069575559817 # based on mean\n",
        "    for i in range(len(testData)):\n",
        "      predictedTSDOW[randGeoHash] =(getNaive(meanTrainDataBy_TSDOW, (testData.iloc[i].timestamp, testData.iloc[i].dayOfWeek), defaultDemand))\n",
        "      predictedTS[randGeoHash] = (getNaive(meanTrainDataBy_TS, (testData.iloc[i].timestamp), defaultDemand))\n",
        "      predictedDOW[randGeoHash] = (getNaive(meanTrainDataBy_DOW, (testData.iloc[i].dayOfWeek), defaultDemand))\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for naive \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 1 - auto arima\n",
        "  #https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n",
        "  # TODO auto-arima use step rather than respective X\n",
        "  if(boolSectionsToRun[1]):    \n",
        "    try:\n",
        "      arima_model = auto_arima(trainData.demand, error_action='ignore', suppress_warnings=True)        \n",
        "      model_fit = arima_model.fit(trainData.demand, suppress_warnings=True)\n",
        "      preds['predictedArima1'][randGeoHash] = predictInvalidPreds(model_fit.predict(n_periods=len(testData)), randGeoHash)\n",
        "      # seem like bug in auto_arima library for geohash6 qp09jy. too many indices when run something like this arima_model1=auto_arima([0.03,0.35])\n",
        "    except IndexError:\n",
        "      print('unexpected indexerror in arima')\n",
        "      preds['predictedArima1'][randGeoHash]=predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    # assumption that the last 5 data point is of most importance\n",
        "    try:\n",
        "      arima_model2 = auto_arima(trainData[-5:].demand, suppress_warnings=True)\n",
        "      model_fit2 = arima_model2.fit(trainData[-5:].demand, suppress_warnings=True)\n",
        "      preds['predictedArima2'][randGeoHash] = predictInvalidPreds(model_fit2.predict(n_periods=len(testData)), randGeoHash)\n",
        "    except IndexError:\n",
        "      print('unexpected indexerror in arima')\n",
        "      preds['predictedArima2'][randGeoHash]=predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    # assumption 7 days * 24hrs * 4/hour records per season\n",
        "    #arima_model3 = auto_arima(trainData.demand, error_action=m=7*24*4, suppress_warnings=True)\n",
        "    #model_fit3 = arima_model3.fit(trainData.demand, suppress_warnings=True)\n",
        "    #predictedArima3[randGeoHash] = predictInvalidPreds(model_fit3.predict(n_periods=len(testData)), randGeoHash)\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for auto arima \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 2 - xgboost and others\n",
        "  #https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "  #https://www.kaggle.com/mburakergenc/predictions-with-xgboost-and-linear-regression\n",
        "  #https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6 on tuning gamma\n",
        "  if(boolSectionsToRun[2]):\n",
        "    xg_reg1 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000, silent=True)\n",
        "    xg_reg1.fit(trainData10, trainLabel10)\n",
        "    preds['predictedXGBoost1'][randGeoHash] = predictInvalidPreds(xg_reg1.predict(testData10), randGeoHash)\n",
        "\n",
        "    # not sure why but this is significant different than others\n",
        "    xg_reg2 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True)\n",
        "    xg_reg2.fit(trainData1, trainLabel1)\n",
        "    preds['predictedXGBoost2'][randGeoHash] = predictInvalidPreds(xg_reg2.predict(testData1), randGeoHash)\n",
        "    \n",
        "    # from gridsearch except n_estimators = 1000, etc\n",
        "    #xg_reg3 = xgb.XGBRegressor(n_estimators = 1000, alpha=0, colsample_bytree=0.3, gamma=0.5, learning_rate=0.1, max_depth=7, min_child_weight=10, subsample=1.0,silent=True)\n",
        "    #xg_reg3.fit(trainData1, trainLabel1)\n",
        "    #preds['predictedXGBoost3'][randGeoHash] = predictInvalidPreds(xg_reg3.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #xg_reg4 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #    min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg4.fit(trainData2, trainLabel2)\n",
        "    #preds['predictedXGBoost4'][randGeoHash] = predictInvalidPreds(xg_reg4.predict(testData2), randGeoHash)\n",
        "    \n",
        "    # just to confirm model ok (use demand to predict demand)\n",
        "    #xg_reg5 = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7, silent=True)\n",
        "    #xg_reg5.fit(trainDataForTest, trainData.demand)\n",
        "    #preds['predictedXGBoost5-perfect'][randGeoHash] = predictInvalidPreds(xg_reg5.predict(testDataForTest), randGeoHash)\n",
        "\n",
        "    #xg_reg6 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #     min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg6.fit(trainData3, trainLabel3)\n",
        "    #preds['predictedXGBoost6'][randGeoHash] = predictInvalidPreds(xg_reg6.predict(testData3), randGeoHash)\n",
        "    \n",
        "    #xg_reg7 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #     min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg7.fit(trainData4, trainLabel4)\n",
        "    #preds['predictedXGBoost7-testData4'][randGeoHash] = predictInvalidPreds(xg_reg7.predict(testData4), randGeoHash)\n",
        "    \n",
        "    #xg_reg8.fit(trainData1, trainLabel1)\n",
        "    #preds['predictedXGBoost8'][randGeoHash] = predictInvalidPreds(xg_reg8.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #xg_reg9 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True).fit(trainData9, trainLabel9)\n",
        "    #preds['predictedXGBoost9'][randGeoHash] = predictInvalidPreds(xg_reg9.predict(testData9), randGeoHash)\n",
        "    \n",
        "    \n",
        "    #https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.d65q9kG1.dpuf\n",
        "    gbrt=GradientBoostingRegressor(n_estimators=100).fit(trainData1, trainLabel1)\n",
        "    preds['gradientBoostRegressor1'][randGeoHash] = predictInvalidPreds(gbrt.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    \n",
        "    #gbrt2=GradientBoostingRegressor(n_estimators=100).fit(trainData7, trainLabel7)\n",
        "    #preds['gradientBoostRegressor2'][randGeoHash] = predictInvalidPreds(gbrt2.predict(testData7), randGeoHash)    \n",
        "     \n",
        "      \n",
        "    #gbrt3.fit(trainData1, trainLabel1)\n",
        "    #preds['gradientBoostRegressor3'][randGeoHash] = predictInvalidPreds(gbrt3.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    # from gridsearch based on gbrt1\n",
        "    gbrt4=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)      \n",
        "    gbrt4.fit(trainData1, trainLabel1)\n",
        "    preds['gradientBoostRegressor4'][randGeoHash] = predictInvalidPreds(gbrt4.predict(testData1), randGeoHash)    \n",
        "        \n",
        "    #gbrt5=GradientBoostingRegressor(n_estimators=1000, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)\n",
        "    #gbrt5.fit(trainData1, trainLabel1)\n",
        "    #preds['gradientBoostRegressor5'][randGeoHash] = predictInvalidPreds(gbrt5.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    gbrt6=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,subsample=0.8).fit(trainData9, trainLabel9)\n",
        "    preds['gradientBoostRegressor6'][randGeoHash] = predictInvalidPreds(gbrt6.predict(testData9), randGeoHash)  \n",
        "    \n",
        "    # was trying to use different param more estimator for larger train data\n",
        "    gbrt7=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)\n",
        "    gbrt7.fit(trainData1, trainLabel1)\n",
        "    preds['gradientBoostRegressor7'][randGeoHash] = predictInvalidPreds(gbrt7.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    # average ensemble of my 2 best \n",
        "    preds['ensembleXGBoost2Gradient4'][randGeoHash] = (preds['predictedXGBoost2'][randGeoHash]+preds['gradientBoostRegressor7'][randGeoHash])/2\n",
        "    \n",
        "    #from catboost import CatBoostRegressor\n",
        "    #cat1 = CatBoostRegressor(iterations=1000,learning_rate=1,depth=2,silent=True)\n",
        "    #cat1.fit(trainData1, trainLabel1)\n",
        "    #preds['catBoost1'][randGeoHash] = predictInvalidPreds(cat1.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    #from sklearn.linear_model import LogisticRegression\n",
        "    # need to change shape and not done\n",
        "    #logReg = LogisticRegression().fit(trainData1, testData1)\n",
        "    #preds['logReg'][randGeoHash] = predictInvalidPreds(logReg.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "    #gpr = GaussianProcessRegressor(normalize_y=True).fit(trainData1, trainLabel1)\n",
        "    #preds['gaussian'][randGeoHash] = predictInvalidPreds(gpr.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    #from sklearn.linear_model import SGDRegressor\n",
        "    #sgdr=SGDRegressor(max_iter=1000, tol=1e-3).fit(trainData1, trainLabel1)\n",
        "    #preds['SGDR'][randGeoHash] = predictInvalidPreds(gpr.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    #https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
        "    #import lightgbm as lgb\n",
        "    #train_data=lgb.Dataset(trainData1,label=trainLabel1)\n",
        "    #params = {'learning_rate':0.001}\n",
        "    #model= lgb.train(params, train_data, 100)\n",
        "    #preds['lgm1'][randGeoHash] = predictInvalidPreds(model.predict(testData1), randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for xgboost \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 3 - TBATS\n",
        "  #https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a\n",
        "  if(boolSectionsToRun[3]):\n",
        "    estimator = TBATS()  \n",
        "    fitted_model = estimator.fit(trainData.demand)  \n",
        "    preds['predictedTBATS'][randGeoHash] = predictInvalidPreds(fitted_model.forecast(steps=len(testData)), randGeoHash)\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for TBATS \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 4 - Random Forest\n",
        "  #https://github.com/sdaulton/TaxiPrediction/blob/master/5b.%20Destinations%20-%20Random%20Forest.ipynb\n",
        "  if(boolSectionsToRun[4]):\n",
        "    #reg1 = RandomForestRegressor(n_estimators=1, max_depth=20, n_jobs=-1,warm_start=True) \n",
        "    #reg1.fit(trainData1,trainLabel1) \n",
        "    #preds['predictedRandForestRegressor1'][randGeoHash] = predictInvalidPreds(reg1.predict(testData1), randGeoHash)\n",
        "\n",
        "    #reg2 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True) \n",
        "    #reg2.fit(trainData1,trainLabel1) \n",
        "    # preds['predictedRandForestRegressor2'][randGeoHash] = predictInvalidPreds(reg2.predict(testData1), randGeoHash)\n",
        "\n",
        "    reg3 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    reg3.fit(trainData1,trainLabel1)\n",
        "    preds['predictedRandForestRegressor3'][randGeoHash] = predictInvalidPreds(reg3.predict(testData1), randGeoHash)\n",
        "    \n",
        "    reg4 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)\n",
        "    reg4.fit(trainDataForTest,trainData.demand)\n",
        "    preds['predictedRandForestRegressor4-perfect'][randGeoHash] = predictInvalidPreds(reg4.predict(testDataForTest), randGeoHash)\n",
        "    \n",
        "    #reg5 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg5.fit(trainData2,trainLabel2)\n",
        "    #preds['predictedRandForestRegressor5'][randGeoHash] = predictInvalidPreds(reg5.predict(testData2), randGeoHash)\n",
        "    \n",
        "    #reg6 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg6.fit(trainData3,trainLabel3)\n",
        "    #preds['predictedRandForestRegressor6'][randGeoHash] = predictInvalidPreds(reg6.predict(testData3), randGeoHash)\n",
        "    \n",
        "    #reg7 = RandomForestRegressor(n_estimators=100, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg7.fit(trainData4,trainLabel4)\n",
        "    #preds['predictedRandForestRegressor7'][randGeoHash] = predictInvalidPreds(reg7.predict(testData4), randGeoHash)\n",
        "        \n",
        "    reg8.fit(trainData1,trainLabel1)\n",
        "    preds['predictedRandForestRegressor8'][randGeoHash] = predictInvalidPreds(reg8.predict(testData1), randGeoHash)\n",
        "    \n",
        "\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for randforest \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "   # Section 5 - keras RNN \n",
        "   #https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
        "  if(boolSectionsToRun[5]):\n",
        "    train5X,train5Y,test5X = create_RNNset(trainData5, trainLabel5, testData5)     \n",
        "    \n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #train4X = scaler.fit_transform(dataset)\n",
        "    #test4X = scaler.fit_transform(dataset)        \n",
        "    #test4Y = scaler.inverse_transform([testY])\n",
        "    \n",
        "    #keras1 = Sequential()\n",
        "    #keras1.add(LSTM(4, input_shape=(1, 1)))\n",
        "    #keras1.add(Dense(1))\n",
        "    #keras1.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    #keras1.fit(train5X, train5Y, epochs=5, batch_size=1, verbose=0)    \n",
        "    #preds['keras1'][randGeoHash] = predictInvalidPreds(keras1.predict(test5X), randGeoHash)\n",
        "    \n",
        "    keras2.fit(train5X, train5Y, epochs=5, batch_size=1, verbose=0)    \n",
        "    preds['keras2'][randGeoHash] = predictInvalidPreds(keras2.predict(test5X), randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for keras \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "   \n",
        "  if(boolSectionsToRun[6]):\n",
        "    m = Prophet()\n",
        "    trainData['ds']=trainData['datetime']\n",
        "    trainData['cap']=1\n",
        "    trainData['y']=trainData['demand']\n",
        "    testData['ds']=testData['datetime']\n",
        "    testData['cap']=1\n",
        "    \n",
        "    m.fit(trainData)\n",
        "    preds['fb-prophet'][randGeoHash] = m.predict(testData)\n",
        "    preds['fb-prophet'][randGeoHash] = predictInvalidPreds(preds['fb-prophet'][randGeoHash]['yhat'].values, randGeoHash)\n",
        "    \n",
        "    m2 = Prophet(growth = 'logistic')\n",
        "    m2.fit(trainData)\n",
        "    preds['fb-prophet2'][randGeoHash] = m2.predict(testData)\n",
        "    preds['fb-prophet2'][randGeoHash] = predictInvalidPreds(preds['fb-prophet2'][randGeoHash]['yhat'].values, randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for fb-prophet \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "# Result comparison at end\n",
        "# just loop all and print if a working hash was not empty\n",
        "for pred in tests:\n",
        "    if(len(preds[pred][workingHashForPrinting]) > 0):\n",
        "      avgRMSE(preds[pred], testDatas, pred)\n",
        "\n",
        "# run one more time using fully trained models, result was worse off\n",
        "trainedpreds={}\n",
        "trainedTests=['gbrt1grid']#'gbrt3','keras2'] #'reg8','xg_reg8'\n",
        "\n",
        "# see if regressor can be improved \n",
        "#adjRMSE(preds['gradientBoostRegressor4'],testDatas,'gbrt4 adj')\n",
        "#adjRMSE(preds['gradientBoostRegressor5'],testDatas,'gbrt5 adj')\n",
        "\n",
        "# not running as no improvement from a general model\n",
        "if(False):\n",
        "  for pred in trainedTests:\n",
        "    trainedpreds[pred]={}\n",
        "\n",
        "  #See more at: https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.Bwu6NdCJ.dpuf\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n",
        "    param_grid = {'n_estimators':range(20,81,10), 'max_depth':range(1,20)}, n_jobs=4,iid=False, cv=5)\n",
        "  \n",
        "  \n",
        "  for randGeoHash in randGeoHashes:  \n",
        "    randDataSet=dataset[dataset.geohash6==randGeoHash]\n",
        "    testData=randDataSet[-5:]\n",
        "    trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]  \n",
        "    if(len(randDataSet) <= 5 or len(trainData) <= 1):\n",
        "      result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "      for pred in trainedpreds:\n",
        "        trainedpreds[pred][randGeoHash]=result\n",
        "      continue  \n",
        "          \n",
        "    \n",
        "    trainData1,trainLabel1,testData1=splitData1(dataset[dataset.geohash6==randGeoHash], dataset)      \n",
        "    gsearch1.fit(trainData1,trainLabel1)\n",
        "    #trainedpreds['gbrt3'][randGeoHash]=gbrt3.predict(testData1)\n",
        "\n",
        "    #trainData5,trainLabel5,testData5=splitData1(randDataSet, dataset)      \n",
        "    #train5X,train5Y,test5X = create_RNNset(trainData5, trainLabel5, testData5) \n",
        "    #trainedpreds['keras2'][randGeoHash] = predictInvalidPreds(keras2.predict(test5X), randGeoHash)\n",
        "\n",
        "  #for pred in trainedTests:    \n",
        "  #  avgRMSE(trainedpreds[pred], testDatas, pred)\n",
        "\n",
        "  \n",
        "# Comments\n",
        "# AutoArima / TBATS slow and couldn't put in missing data logically. tried RNN, FB Prope in others pyb and was slow\n",
        "# using tree for time series is weird too\n"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "geohash is  qp02yc\n",
            "updating negative prediction of  -0.002261800114990512  for geohash  qp02yc  to 0\n",
            "geohash is  qp02yf\n",
            "updating negative prediction of  -0.00037521124  for geohash  qp02yf  to 0\n",
            "updating negative prediction of  -7.134676e-05  for geohash  qp02yf  to 0\n",
            "geohash is  qp02yu\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp02yv\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp02yy\n",
            "updating negative prediction of  -0.00014895201  for geohash  qp02yy  to 0\n",
            "updating negative prediction of  -0.00011217594  for geohash  qp02yy  to 0\n",
            "updating negative prediction of  -8.106232e-05  for geohash  qp02yy  to 0\n",
            "geohash is  qp02yz\n",
            "geohash is  qp02z1\n",
            "geohash is  qp02z3\n",
            "geohash is  qp02z4\n",
            "geohash is  qp02z5\n",
            "geohash is  qp02z6\n",
            "updating negative prediction of  -0.006103754  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.005628228  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.0033970475  for geohash  qp02z6  to 0\n",
            "geohash is  qp02z7\n",
            "geohash is  qp02z9\n",
            "geohash is  qp02zc\n",
            "updating negative prediction of  -0.008777022  for geohash  qp02zc  to 0\n",
            "geohash is  qp02zd\n",
            "updating negative prediction of  -0.011222541  for geohash  qp02zd  to 0\n",
            "updating negative prediction of  -0.0079315305  for geohash  qp02zd  to 0\n",
            "updating negative prediction of  -0.0015979309193045537  for geohash  qp02zd  to 0\n",
            "geohash is  qp02ze\n",
            "updating negative prediction of  -0.014687598  for geohash  qp02ze  to 0\n",
            "updating negative prediction of  -0.01428026  for geohash  qp02ze  to 0\n",
            "geohash is  qp02zf\n",
            "geohash is  qp02zg\n",
            "geohash is  qp02zh\n",
            "updating negative prediction of  -0.011052489  for geohash  qp02zh  to 0\n",
            "updating negative prediction of  -0.01232636  for geohash  qp02zh  to 0\n",
            "updating negative prediction of  -0.017567933  for geohash  qp02zh  to 0\n",
            "geohash is  qp02zj\n",
            "updating negative prediction of  -0.015937507  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.027340949  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.034136295  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.0046875477  for geohash  qp02zj  to 0\n",
            "geohash is  qp02zk\n",
            "updating negative prediction of  -0.0008595586  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.016199768  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.016199768  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.012077749  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.017977536  for geohash  qp02zk  to 0\n",
            "geohash is  qp02zm\n",
            "geohash is  qp02zn\n",
            "geohash is  qp02zp\n",
            "geohash is  qp02zq\n",
            "geohash is  qp02zr\n",
            "updating negative prediction of  -0.008030236  for geohash  qp02zr  to 0\n",
            "updating negative prediction of  -0.015254557  for geohash  qp02zr  to 0\n",
            "geohash is  qp02zs\n",
            "geohash is  qp02zt\n",
            "updating negative prediction of  -0.0037279725  for geohash  qp02zt  to 0\n",
            "geohash is  qp02zu\n",
            "geohash is  qp02zv\n",
            "geohash is  qp02zw\n",
            "geohash is  qp02zx\n",
            "geohash is  qp02zy\n",
            "geohash is  qp02zz\n",
            "geohash is  qp03jq\n",
            "geohash is  qp03jr\n",
            "geohash is  qp03jw\n",
            "geohash is  qp03jx\n",
            "geohash is  qp03jy\n",
            "geohash is  qp03jz\n",
            "geohash is  qp03m2\n",
            "geohash is  qp03m3\n",
            "geohash is  qp03m6\n",
            "geohash is  qp03m7\n",
            "geohash is  qp03m8\n",
            "geohash is  qp03m9\n",
            "geohash is  qp03mb\n",
            "geohash is  qp03mc\n",
            "geohash is  qp03md\n",
            "geohash is  qp03me\n",
            "geohash is  qp03mf\n",
            "geohash is  qp03mg\n",
            "geohash is  qp03mk\n",
            "geohash is  qp03mm\n",
            "geohash is  qp03mq\n",
            "geohash is  qp03mr\n",
            "geohash is  qp03ms\n",
            "geohash is  qp03mt\n",
            "geohash is  qp03mu\n",
            "updating negative prediction of  -0.00011101530381586405  for geohash  qp03mu  to 0\n",
            "geohash is  qp03mv\n",
            "geohash is  qp03mw\n",
            "geohash is  qp03mx\n",
            "geohash is  qp03my\n",
            "geohash is  qp03mz\n",
            "geohash is  qp03nb\n",
            "geohash is  qp03nd\n",
            "geohash is  qp03nf\n",
            "geohash is  qp03nn\n",
            "geohash is  qp03np\n",
            "geohash is  qp03nq\n",
            "geohash is  qp03nr\n",
            "geohash is  qp03nw\n",
            "geohash is  qp03nx\n",
            "geohash is  qp03ny\n",
            "geohash is  qp03nz\n",
            "geohash is  qp03p0\n",
            "updating negative prediction of  -0.0029454231  for geohash  qp03p0  to 0\n",
            "geohash is  qp03p1\n",
            "geohash is  qp03p2\n",
            "updating negative prediction of  -0.0077913404  for geohash  qp03p2  to 0\n",
            "geohash is  qp03p3\n",
            "geohash is  qp03p4\n",
            "updating negative prediction of  -0.002785802  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.015069008  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.007893562  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.017463565  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.015919924  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.0015024543  for geohash  qp03p4  to 0\n",
            "updating negative prediction of  -0.0032055974  for geohash  qp03p4  to 0\n",
            "geohash is  qp03p5\n",
            "geohash is  qp03p6\n",
            "geohash is  qp03p7\n",
            "updating negative prediction of  -0.001426816  for geohash  qp03p7  to 0\n",
            "updating negative prediction of  -0.025330305  for geohash  qp03p7  to 0\n",
            "updating negative prediction of  -0.034204602  for geohash  qp03p7  to 0\n",
            "updating negative prediction of  -0.017923295  for geohash  qp03p7  to 0\n",
            "updating negative prediction of  -0.022819996  for geohash  qp03p7  to 0\n",
            "geohash is  qp03p8\n",
            "geohash is  qp03p9\n",
            "updating negative prediction of  -0.010399163  for geohash  qp03p9  to 0\n",
            "updating negative prediction of  -0.013837516  for geohash  qp03p9  to 0\n",
            "updating negative prediction of  -0.009234144914477353  for geohash  qp03p9  to 0\n",
            "updating negative prediction of  -0.008364553029455647  for geohash  qp03p9  to 0\n",
            "geohash is  qp03pb\n",
            "geohash is  qp03pc\n",
            "geohash is  qp03pd\n",
            "updating negative prediction of  -0.0021776557  for geohash  qp03pd  to 0\n",
            "geohash is  qp03pe\n",
            "geohash is  qp03pf\n",
            "geohash is  qp03pg\n",
            "updating negative prediction of  -0.00054603815  for geohash  qp03pg  to 0\n",
            "updating negative prediction of  -0.00047010183  for geohash  qp03pg  to 0\n",
            "updating negative prediction of  -0.00060391426  for geohash  qp03pg  to 0\n",
            "geohash is  qp03pj\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp03pk\n",
            "geohash is  qp03pm\n",
            "updating negative prediction of  -0.0012618899  for geohash  qp03pm  to 0\n",
            "geohash is  qp03pn\n",
            "updating negative prediction of  -0.013533711  for geohash  qp03pn  to 0\n",
            "updating negative prediction of  -0.0125309825  for geohash  qp03pn  to 0\n",
            "updating negative prediction of  -0.0125309825  for geohash  qp03pn  to 0\n",
            "updating negative prediction of  -0.0011396408  for geohash  qp03pn  to 0\n",
            "updating negative prediction of  -0.00086420774  for geohash  qp03pn  to 0\n",
            "updating negative prediction of  -0.001070559  for geohash  qp03pn  to 0\n",
            "geohash is  qp03pp\n",
            "geohash is  qp03pq\n",
            "geohash is  qp03pr\n",
            "geohash is  qp03ps\n",
            "geohash is  qp03pt\n",
            "updating negative prediction of  -0.0005943775  for geohash  qp03pt  to 0\n",
            "updating negative prediction of  -0.001552999  for geohash  qp03pt  to 0\n",
            "geohash is  qp03pu\n",
            "geohash is  qp03pv\n",
            "geohash is  qp03pw\n",
            "geohash is  qp03px\n",
            "geohash is  qp03py\n",
            "geohash is  qp03pz\n",
            "geohash is  qp03q0\n",
            "geohash is  qp03q1\n",
            "geohash is  qp03q2\n",
            "geohash is  qp03q3\n",
            "geohash is  qp03q4\n",
            "geohash is  qp03q5\n",
            "geohash is  qp03q6\n",
            "geohash is  qp03q7\n",
            "geohash is  qp03q8\n",
            "geohash is  qp03q9\n",
            "geohash is  qp03qb\n",
            "geohash is  qp03qc\n",
            "geohash is  qp03qd\n",
            "geohash is  qp03qe\n",
            "geohash is  qp03qf\n",
            "geohash is  qp03qg\n",
            "geohash is  qp03qh\n",
            "geohash is  qp03qj\n",
            "geohash is  qp03qk\n",
            "geohash is  qp03qm\n",
            "geohash is  qp03qn\n",
            "updating negative prediction of  -0.0008036494  for geohash  qp03qn  to 0\n",
            "updating negative prediction of  -0.00065499544  for geohash  qp03qn  to 0\n",
            "updating negative prediction of  -0.0008036494  for geohash  qp03qn  to 0\n",
            "geohash is  qp03qp\n",
            "geohash is  qp03qq\n",
            "geohash is  qp03qr\n",
            "geohash is  qp03qs\n",
            "geohash is  qp03qt\n",
            "geohash is  qp03qu\n",
            "geohash is  qp03qv\n",
            "geohash is  qp03qw\n",
            "updating negative prediction of  -0.0009986162  for geohash  qp03qw  to 0\n",
            "geohash is  qp03qx\n",
            "geohash is  qp03qy\n",
            "geohash is  qp03qz\n",
            "geohash is  qp03r0\n",
            "geohash is  qp03r1\n",
            "geohash is  qp03r2\n",
            "geohash is  qp03r3\n",
            "geohash is  qp03r4\n",
            "geohash is  qp03r5\n",
            "geohash is  qp03r6\n",
            "geohash is  qp03r7\n",
            "geohash is  qp03r8\n",
            "geohash is  qp03r9\n",
            "geohash is  qp03rb\n",
            "geohash is  qp03rc\n",
            "geohash is  qp03rd\n",
            "geohash is  qp03re\n",
            "geohash is  qp03rf\n",
            "geohash is  qp03rg\n",
            "geohash is  qp03rh\n",
            "geohash is  qp03rj\n",
            "geohash is  qp03rk\n",
            "geohash is  qp03rm\n",
            "geohash is  qp03rn\n",
            "geohash is  qp03rp\n",
            "geohash is  qp03rq\n",
            "geohash is  qp03rr\n",
            "geohash is  qp03rs\n",
            "geohash is  qp03rt\n",
            "geohash is  qp03ru\n",
            "geohash is  qp03rv\n",
            "geohash is  qp03rw\n",
            "geohash is  qp03rx\n",
            "geohash is  qp03ry\n",
            "geohash is  qp03rz\n",
            "geohash is  qp03t2\n",
            "geohash is  qp03t3\n",
            "geohash is  qp03t6\n",
            "geohash is  qp03t7\n",
            "geohash is  qp03t8\n",
            "geohash is  qp03t9\n",
            "geohash is  qp03tb\n",
            "geohash is  qp03tc\n",
            "geohash is  qp03td\n",
            "geohash is  qp03te\n",
            "geohash is  qp03tf\n",
            "geohash is  qp03tg\n",
            "geohash is  qp03tk\n",
            "geohash is  qp03tm\n",
            "geohash is  qp03tq\n",
            "geohash is  qp03tr\n",
            "geohash is  qp03ts\n",
            "geohash is  qp03tt\n",
            "geohash is  qp03tu\n",
            "geohash is  qp03tv\n",
            "geohash is  qp03tw\n",
            "geohash is  qp03tx\n",
            "geohash is  qp03ty\n",
            "geohash is  qp03tz\n",
            "geohash is  qp03v2\n",
            "geohash is  qp03v3\n",
            "geohash is  qp03v9\n",
            "geohash is  qp03vb\n",
            "geohash is  qp03vc\n",
            "geohash is  qp03vd\n",
            "geohash is  qp03vf\n",
            "geohash is  qp03w0\n",
            "geohash is  qp03w1\n",
            "geohash is  qp03w2\n",
            "geohash is  qp03w3\n",
            "geohash is  qp03w4\n",
            "geohash is  qp03w5\n",
            "geohash is  qp03w6\n",
            "geohash is  qp03w7\n",
            "geohash is  qp03w8\n",
            "geohash is  qp03w9\n",
            "geohash is  qp03wb\n",
            "updating negative prediction of  -0.002965629  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.006515503  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.0070200562  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.0063058734  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.02044034  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.000708282  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -0.0033755898  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -2.3998708852721647e-05  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -2.3998708852721647e-05  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -2.3998708852721647e-05  for geohash  qp03wb  to 0\n",
            "updating negative prediction of  -2.3998708852721647e-05  for geohash  qp03wb  to 0\n",
            "geohash is  qp03wc\n",
            "geohash is  qp03wd\n",
            "geohash is  qp03we\n",
            "geohash is  qp03wf\n",
            "geohash is  qp03wg\n",
            "geohash is  qp03wh\n",
            "geohash is  qp03wj\n",
            "geohash is  qp03wk\n",
            "geohash is  qp03wm\n",
            "geohash is  qp03wn\n",
            "geohash is  qp03wp\n",
            "geohash is  qp03wq\n",
            "geohash is  qp03wr\n",
            "geohash is  qp03ws\n",
            "geohash is  qp03wt\n",
            "geohash is  qp03wu\n",
            "geohash is  qp03wv\n",
            "geohash is  qp03ww\n",
            "geohash is  qp03wx\n",
            "geohash is  qp03wy\n",
            "geohash is  qp03wz\n",
            "geohash is  qp03x0\n",
            "updating negative prediction of  -0.00327003  for geohash  qp03x0  to 0\n",
            "updating negative prediction of  -0.011003733  for geohash  qp03x0  to 0\n",
            "updating negative prediction of  -0.010028064  for geohash  qp03x0  to 0\n",
            "updating negative prediction of  -0.010046303  for geohash  qp03x0  to 0\n",
            "updating negative prediction of  -7.432699e-05  for geohash  qp03x0  to 0\n",
            "updating negative prediction of  -9.030104e-05  for geohash  qp03x0  to 0\n",
            "geohash is  qp03x1\n",
            "geohash is  qp03x2\n",
            "geohash is  qp03x3\n",
            "geohash is  qp03x4\n",
            "geohash is  qp03x5\n",
            "geohash is  qp03x6\n",
            "geohash is  qp03x7\n",
            "geohash is  qp03x8\n",
            "geohash is  qp03x9\n",
            "geohash is  qp03xb\n",
            "geohash is  qp03xc\n",
            "geohash is  qp03xd\n",
            "geohash is  qp03xe\n",
            "geohash is  qp03xf\n",
            "geohash is  qp03xg\n",
            "geohash is  qp03xh\n",
            "geohash is  qp03xj\n",
            "geohash is  qp03xk\n",
            "geohash is  qp03xm\n",
            "geohash is  qp03xn\n",
            "geohash is  qp03xp\n",
            "geohash is  qp03xq\n",
            "updating negative prediction of  -0.013692368955793652  for geohash  qp03xq  to 0\n",
            "updating negative prediction of  -0.013692368955793652  for geohash  qp03xq  to 0\n",
            "updating negative prediction of  -0.013692368955793652  for geohash  qp03xq  to 0\n",
            "updating negative prediction of  -0.013692368955793652  for geohash  qp03xq  to 0\n",
            "updating negative prediction of  -0.013692368955793652  for geohash  qp03xq  to 0\n",
            "geohash is  qp03xr\n",
            "geohash is  qp03xs\n",
            "geohash is  qp03xt\n",
            "geohash is  qp03xu\n",
            "geohash is  qp03xv\n",
            "geohash is  qp03xw\n",
            "geohash is  qp03xx\n",
            "geohash is  qp03xy\n",
            "geohash is  qp03xz\n",
            "geohash is  qp03y0\n",
            "geohash is  qp03y1\n",
            "geohash is  qp03y2\n",
            "geohash is  qp03y3\n",
            "geohash is  qp03y4\n",
            "geohash is  qp03y5\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp03y6\n",
            "geohash is  qp03y7\n",
            "geohash is  qp03y8\n",
            "geohash is  qp03y9\n",
            "geohash is  qp03yb\n",
            "geohash is  qp03yc\n",
            "geohash is  qp03yd\n",
            "geohash is  qp03ye\n",
            "geohash is  qp03yf\n",
            "geohash is  qp03yg\n",
            "geohash is  qp03yh\n",
            "geohash is  qp03yk\n",
            "geohash is  qp03ym\n",
            "geohash is  qp03yn\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp03yq\n",
            "geohash is  qp03yr\n",
            "geohash is  qp03ys\n",
            "geohash is  qp03yt\n",
            "geohash is  qp03yu\n",
            "geohash is  qp03yv\n",
            "geohash is  qp03yw\n",
            "geohash is  qp03yx\n",
            "geohash is  qp03yy\n",
            "geohash is  qp03yz\n",
            "geohash is  qp03z0\n",
            "geohash is  qp03z1\n",
            "geohash is  qp03z2\n",
            "geohash is  qp03z3\n",
            "geohash is  qp03z4\n",
            "geohash is  qp03z5\n",
            "updating negative prediction of  -0.00012540817  for geohash  qp03z5  to 0\n",
            "updating negative prediction of  -0.01722598  for geohash  qp03z5  to 0\n",
            "updating negative prediction of  -0.01722598  for geohash  qp03z5  to 0\n",
            "updating negative prediction of  -0.008963287  for geohash  qp03z5  to 0\n",
            "updating negative prediction of  -0.010847867  for geohash  qp03z5  to 0\n",
            "updating negative prediction of  -0.006962657  for geohash  qp03z5  to 0\n",
            "geohash is  qp03z6\n",
            "geohash is  qp03z7\n",
            "geohash is  qp03z8\n",
            "geohash is  qp03z9\n",
            "geohash is  qp03zb\n",
            "geohash is  qp03zc\n",
            "geohash is  qp03zd\n",
            "geohash is  qp03ze\n",
            "geohash is  qp03zf\n",
            "geohash is  qp03zg\n",
            "geohash is  qp03zh\n",
            "geohash is  qp03zj\n",
            "geohash is  qp03zk\n",
            "updating negative prediction of  -0.0030105114  for geohash  qp03zk  to 0\n",
            "updating negative prediction of  -0.0017786622  for geohash  qp03zk  to 0\n",
            "updating negative prediction of  -0.001550227082347585  for geohash  qp03zk  to 0\n",
            "updating negative prediction of  -0.00043045591465484204  for geohash  qp03zk  to 0\n",
            "geohash is  qp03zm\n",
            "geohash is  qp03zn\n",
            "geohash is  qp03zp\n",
            "geohash is  qp03zq\n",
            "geohash is  qp03zr\n",
            "geohash is  qp03zs\n",
            "geohash is  qp03zt\n",
            "geohash is  qp03zu\n",
            "geohash is  qp03zv\n",
            "geohash is  qp03zw\n",
            "geohash is  qp03zx\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp03zy\n",
            "geohash is  qp03zz\n",
            "geohash is  qp06n8\n",
            "geohash is  qp06n9\n",
            "geohash is  qp06nb\n",
            "geohash is  qp06nc\n",
            "geohash is  qp06nd\n",
            "geohash is  qp06ne\n",
            "geohash is  qp06nf\n",
            "geohash is  qp06ng\n",
            "updating negative prediction of  -0.00040382147  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.00047928095  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.0011352301  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.0012099147  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.00011211634  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.00062328577  for geohash  qp06ng  to 0\n",
            "updating negative prediction of  -0.00069510937  for geohash  qp06ng  to 0\n",
            "geohash is  qp06ns\n",
            "geohash is  qp06nt\n",
            "geohash is  qp06nu\n",
            "geohash is  qp06nv\n",
            "geohash is  qp06nw\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp06ny\n",
            "geohash is  qp06p0\n",
            "geohash is  qp06p1\n",
            "geohash is  qp06p2\n",
            "geohash is  qp06p3\n",
            "geohash is  qp06p4\n",
            "geohash is  qp06p5\n",
            "geohash is  qp06p6\n",
            "geohash is  qp06p7\n",
            "geohash is  qp06p8\n",
            "updating negative prediction of  -6.645918e-05  for geohash  qp06p8  to 0\n",
            "geohash is  qp06p9\n",
            "geohash is  qp06pb\n",
            "geohash is  qp06pc\n",
            "geohash is  qp06pd\n",
            "geohash is  qp06pe\n",
            "geohash is  qp06pf\n",
            "geohash is  qp06pg\n",
            "geohash is  qp06ph\n",
            "geohash is  qp06pj\n",
            "geohash is  qp06pk\n",
            "geohash is  qp06pm\n",
            "geohash is  qp06pn\n",
            "updating negative prediction of  -0.0006637573  for geohash  qp06pn  to 0\n",
            "updating negative prediction of  -9.524822e-05  for geohash  qp06pn  to 0\n",
            "updating negative prediction of  -0.0009470582  for geohash  qp06pn  to 0\n",
            "geohash is  qp06pq\n",
            "geohash is  qp06ps\n",
            "geohash is  qp06pt\n",
            "geohash is  qp06pu\n",
            "geohash is  qp06pv\n",
            "geohash is  qp06pw\n",
            "geohash is  qp06py\n",
            "geohash is  qp08b1\n",
            "geohash is  qp08b4\n",
            "geohash is  qp08b5\n",
            "geohash is  qp08b6\n",
            "updating negative prediction of  -0.006394446  for geohash  qp08b6  to 0\n",
            "updating negative prediction of  -0.010697484  for geohash  qp08b6  to 0\n",
            "updating negative prediction of  -0.0059526563  for geohash  qp08b6  to 0\n",
            "geohash is  qp08b7\n",
            "updating negative prediction of  -0.004314542  for geohash  qp08b7  to 0\n",
            "updating negative prediction of  -0.0035287738  for geohash  qp08b7  to 0\n",
            "geohash is  qp08bd\n",
            "updating negative prediction of  -0.0033943653  for geohash  qp08bd  to 0\n",
            "updating negative prediction of  -0.0035749078  for geohash  qp08bd  to 0\n",
            "geohash is  qp08be\n",
            "updating negative prediction of  -0.0007147789  for geohash  qp08be  to 0\n",
            "geohash is  qp08bg\n",
            "geohash is  qp08bh\n",
            "updating negative prediction of  -0.040031075  for geohash  qp08bh  to 0\n",
            "geohash is  qp08bj\n",
            "geohash is  qp08bk\n",
            "geohash is  qp08bm\n",
            "updating negative prediction of  -0.0146071315  for geohash  qp08bm  to 0\n",
            "geohash is  qp08bn\n",
            "geohash is  qp08bp\n",
            "geohash is  qp08bq\n",
            "geohash is  qp08br\n",
            "updating negative prediction of  -0.0076491833  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.009172082  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.020542383  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.014628649  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.00042515993  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.0017544031  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.009772718  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.0015568137  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.006305257310120868  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.006305257310120868  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.0009529305500187658  for geohash  qp08br  to 0\n",
            "updating negative prediction of  -0.011581273745819146  for geohash  qp08br  to 0\n",
            "geohash is  qp08bs\n",
            "geohash is  qp08bt\n",
            "geohash is  qp08bu\n",
            "geohash is  qp08bv\n",
            "updating negative prediction of  -0.003780919907208163  for geohash  qp08bv  to 0\n",
            "updating negative prediction of  -6.795812053331417e-05  for geohash  qp08bv  to 0\n",
            "geohash is  qp08bw\n",
            "geohash is  qp08bx\n",
            "geohash is  qp08by\n",
            "geohash is  qp08bz\n",
            "geohash is  qp08c5\n",
            "geohash is  qp08ch\n",
            "geohash is  qp08cj\n",
            "geohash is  qp08ck\n",
            "updating negative prediction of  -0.0023454428  for geohash  qp08ck  to 0\n",
            "geohash is  qp08cm\n",
            "geohash is  qp08cn\n",
            "geohash is  qp08cp\n",
            "updating negative prediction of  -0.0009825826  for geohash  qp08cp  to 0\n",
            "geohash is  qp08cu\n",
            "geohash is  qp08cv\n",
            "geohash is  qp08cy\n",
            "geohash is  qp08fh\n",
            "geohash is  qp08fj\n",
            "geohash is  qp08fn\n",
            "geohash is  qp08fp\n",
            "geohash is  qp08fq\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp08fr\n",
            "geohash is  qp08fs\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp08ft\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp08fu\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp08fv\n",
            "geohash is  qp08fw\n",
            "geohash is  qp08fx\n",
            "geohash is  qp08fy\n",
            "geohash is  qp08fz\n",
            "geohash is  qp08g4\n",
            "geohash is  qp08g5\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp08g6\n",
            "geohash is  qp08g7\n",
            "geohash is  qp08gh\n",
            "geohash is  qp08gj\n",
            "geohash is  qp08gk\n",
            "geohash is  qp08gm\n",
            "geohash is  qp08gn\n",
            "geohash is  qp08gp\n",
            "geohash is  qp08gq\n",
            "geohash is  qp08gr\n",
            "geohash is  qp08gs\n",
            "geohash is  qp08gt\n",
            "geohash is  qp08gu\n",
            "geohash is  qp08gv\n",
            "geohash is  qp08gw\n",
            "geohash is  qp08gx\n",
            "geohash is  qp08gy\n",
            "geohash is  qp08gz\n",
            "geohash is  qp08uj\n",
            "geohash is  qp08un\n",
            "geohash is  qp08up\n",
            "geohash is  qp0900\n",
            "geohash is  qp0901\n",
            "updating negative prediction of  -0.016279042  for geohash  qp0901  to 0\n",
            "geohash is  qp0902\n",
            "geohash is  qp0903\n",
            "geohash is  qp0904\n",
            "updating negative prediction of  -0.005787909  for geohash  qp0904  to 0\n",
            "updating negative prediction of  -0.005787909  for geohash  qp0904  to 0\n",
            "updating negative prediction of  -0.0057845116  for geohash  qp0904  to 0\n",
            "geohash is  qp0905\n",
            "geohash is  qp0906\n",
            "geohash is  qp0907\n",
            "updating negative prediction of  -0.014080167  for geohash  qp0907  to 0\n",
            "updating negative prediction of  -0.038420796  for geohash  qp0907  to 0\n",
            "updating negative prediction of  -0.035634696  for geohash  qp0907  to 0\n",
            "updating negative prediction of  -0.021371901  for geohash  qp0907  to 0\n",
            "geohash is  qp0908\n",
            "geohash is  qp0909\n",
            "updating negative prediction of  -0.005056381  for geohash  qp0909  to 0\n",
            "updating negative prediction of  -0.0049400926  for geohash  qp0909  to 0\n",
            "geohash is  qp090b\n",
            "updating negative prediction of  -0.0021787286  for geohash  qp090b  to 0\n",
            "updating negative prediction of  -0.0020336509  for geohash  qp090b  to 0\n",
            "geohash is  qp090c\n",
            "updating negative prediction of  -0.008090138  for geohash  qp090c  to 0\n",
            "geohash is  qp090d\n",
            "updating negative prediction of  -0.0042651296  for geohash  qp090d  to 0\n",
            "updating negative prediction of  -0.004908979  for geohash  qp090d  to 0\n",
            "updating negative prediction of  -0.008495867  for geohash  qp090d  to 0\n",
            "updating negative prediction of  -0.0032110214  for geohash  qp090d  to 0\n",
            "geohash is  qp090e\n",
            "geohash is  qp090h\n",
            "updating negative prediction of  -0.009924471  for geohash  qp090h  to 0\n",
            "geohash is  qp090j\n",
            "geohash is  qp090k\n",
            "updating negative prediction of  -0.011196911  for geohash  qp090k  to 0\n",
            "geohash is  qp090m\n",
            "updating negative prediction of  -0.0067822933  for geohash  qp090m  to 0\n",
            "updating negative prediction of  -0.004836321  for geohash  qp090m  to 0\n",
            "updating negative prediction of  -0.00046908855  for geohash  qp090m  to 0\n",
            "updating negative prediction of  -0.0013444424  for geohash  qp090m  to 0\n",
            "geohash is  qp090n\n",
            "geohash is  qp090p\n",
            "updating negative prediction of  -9.6440315e-05  for geohash  qp090p  to 0\n",
            "geohash is  qp090q\n",
            "geohash is  qp090r\n",
            "geohash is  qp090s\n",
            "updating negative prediction of  -0.010196805  for geohash  qp090s  to 0\n",
            "updating negative prediction of  -0.011530817  for geohash  qp090s  to 0\n",
            "updating negative prediction of  -0.010371268  for geohash  qp090s  to 0\n",
            "updating negative prediction of  -0.006768167  for geohash  qp090s  to 0\n",
            "geohash is  qp090t\n",
            "geohash is  qp090v\n",
            "geohash is  qp090w\n",
            "geohash is  qp090x\n",
            "geohash is  qp090y\n",
            "geohash is  qp090z\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0917\n",
            "geohash is  qp0919\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp091d\n",
            "geohash is  qp091e\n",
            "geohash is  qp091f\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp091g\n",
            "geohash is  qp091h\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp091j\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp091k\n",
            "updating negative prediction of  -0.00015697946112367333  for geohash  qp091k  to 0\n",
            "geohash is  qp091m\n",
            "geohash is  qp091n\n",
            "geohash is  qp091p\n",
            "geohash is  qp091q\n",
            "geohash is  qp091r\n",
            "geohash is  qp091s\n",
            "geohash is  qp091t\n",
            "geohash is  qp091u\n",
            "geohash is  qp091v\n",
            "geohash is  qp091w\n",
            "geohash is  qp091x\n",
            "geohash is  qp091y\n",
            "geohash is  qp091z\n",
            "geohash is  qp0920\n",
            "geohash is  qp0921\n",
            "geohash is  qp0922\n",
            "geohash is  qp0923\n",
            "geohash is  qp0924\n",
            "geohash is  qp0925\n",
            "geohash is  qp0926\n",
            "geohash is  qp0927\n",
            "geohash is  qp0928\n",
            "geohash is  qp0929\n",
            "geohash is  qp092b\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp092d\n",
            "geohash is  qp092e\n",
            "geohash is  qp092h\n",
            "geohash is  qp092j\n",
            "geohash is  qp092k\n",
            "geohash is  qp092m\n",
            "geohash is  qp092n\n",
            "geohash is  qp092p\n",
            "geohash is  qp092q\n",
            "geohash is  qp092r\n",
            "geohash is  qp092s\n",
            "geohash is  qp092t\n",
            "geohash is  qp092w\n",
            "geohash is  qp092x\n",
            "geohash is  qp092z\n",
            "geohash is  qp0930\n",
            "geohash is  qp0931\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0932\n",
            "geohash is  qp0933\n",
            "geohash is  qp0934\n",
            "geohash is  qp0935\n",
            "geohash is  qp0936\n",
            "geohash is  qp0937\n",
            "geohash is  qp0938\n",
            "geohash is  qp0939\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp093b\n",
            "geohash is  qp093c\n",
            "geohash is  qp093d\n",
            "geohash is  qp093e\n",
            "geohash is  qp093f\n",
            "geohash is  qp093g\n",
            "geohash is  qp093h\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp093j\n",
            "geohash is  qp093k\n",
            "geohash is  qp093m\n",
            "geohash is  qp093n\n",
            "geohash is  qp093p\n",
            "geohash is  qp093q\n",
            "geohash is  qp093r\n",
            "geohash is  qp093s\n",
            "geohash is  qp093t\n",
            "geohash is  qp093u\n",
            "geohash is  qp093v\n",
            "geohash is  qp093w\n",
            "geohash is  qp093x\n",
            "geohash is  qp093y\n",
            "geohash is  qp093z\n",
            "geohash is  qp0941\n",
            "geohash is  qp0942\n",
            "geohash is  qp0943\n",
            "geohash is  qp0944\n",
            "geohash is  qp0945\n",
            "geohash is  qp0946\n",
            "geohash is  qp0947\n",
            "geohash is  qp0948\n",
            "geohash is  qp0949\n",
            "geohash is  qp094b\n",
            "geohash is  qp094c\n",
            "geohash is  qp094d\n",
            "geohash is  qp094e\n",
            "geohash is  qp094f\n",
            "geohash is  qp094g\n",
            "geohash is  qp094h\n",
            "updating negative prediction of  -0.0139481425  for geohash  qp094h  to 0\n",
            "updating negative prediction of  -0.02798909  for geohash  qp094h  to 0\n",
            "updating negative prediction of  -0.03132969  for geohash  qp094h  to 0\n",
            "updating negative prediction of  -0.050347626  for geohash  qp094h  to 0\n",
            "geohash is  qp094j\n",
            "geohash is  qp094k\n",
            "geohash is  qp094m\n",
            "geohash is  qp094n\n",
            "geohash is  qp094p\n",
            "geohash is  qp094q\n",
            "geohash is  qp094r\n",
            "geohash is  qp094s\n",
            "geohash is  qp094t\n",
            "geohash is  qp094u\n",
            "geohash is  qp094v\n",
            "geohash is  qp094w\n",
            "geohash is  qp094x\n",
            "geohash is  qp094y\n",
            "geohash is  qp094z\n",
            "geohash is  qp0950\n",
            "geohash is  qp0951\n",
            "geohash is  qp0952\n",
            "geohash is  qp0953\n",
            "geohash is  qp0954\n",
            "geohash is  qp0955\n",
            "geohash is  qp0956\n",
            "geohash is  qp0957\n",
            "geohash is  qp0958\n",
            "geohash is  qp0959\n",
            "updating negative prediction of  -0.00066423416  for geohash  qp0959  to 0\n",
            "geohash is  qp095b\n",
            "geohash is  qp095c\n",
            "geohash is  qp095d\n",
            "geohash is  qp095e\n",
            "geohash is  qp095f\n",
            "geohash is  qp095g\n",
            "geohash is  qp095h\n",
            "geohash is  qp095j\n",
            "geohash is  qp095k\n",
            "geohash is  qp095m\n",
            "geohash is  qp095n\n",
            "geohash is  qp095p\n",
            "geohash is  qp095q\n",
            "geohash is  qp095r\n",
            "geohash is  qp095s\n",
            "geohash is  qp095t\n",
            "geohash is  qp095u\n",
            "geohash is  qp095v\n",
            "geohash is  qp095w\n",
            "geohash is  qp095x\n",
            "geohash is  qp095y\n",
            "geohash is  qp095z\n",
            "geohash is  qp0960\n",
            "geohash is  qp0961\n",
            "geohash is  qp0962\n",
            "geohash is  qp0963\n",
            "geohash is  qp0964\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0965\n",
            "geohash is  qp0966\n",
            "geohash is  qp0967\n",
            "geohash is  qp0968\n",
            "geohash is  qp0969\n",
            "geohash is  qp096b\n",
            "geohash is  qp096c\n",
            "geohash is  qp096d\n",
            "geohash is  qp096e\n",
            "geohash is  qp096f\n",
            "geohash is  qp096g\n",
            "geohash is  qp096h\n",
            "geohash is  qp096j\n",
            "geohash is  qp096k\n",
            "geohash is  qp096m\n",
            "geohash is  qp096n\n",
            "geohash is  qp096p\n",
            "geohash is  qp096q\n",
            "geohash is  qp096r\n",
            "geohash is  qp096s\n",
            "geohash is  qp096t\n",
            "geohash is  qp096u\n",
            "geohash is  qp096v\n",
            "geohash is  qp096w\n",
            "geohash is  qp096x\n",
            "geohash is  qp096y\n",
            "geohash is  qp096z\n",
            "geohash is  qp0970\n",
            "geohash is  qp0971\n",
            "geohash is  qp0972\n",
            "geohash is  qp0973\n",
            "geohash is  qp0974\n",
            "geohash is  qp0975\n",
            "geohash is  qp0976\n",
            "geohash is  qp0977\n",
            "geohash is  qp0978\n",
            "geohash is  qp0979\n",
            "geohash is  qp097b\n",
            "geohash is  qp097c\n",
            "geohash is  qp097d\n",
            "geohash is  qp097e\n",
            "geohash is  qp097f\n",
            "geohash is  qp097g\n",
            "geohash is  qp097h\n",
            "geohash is  qp097j\n",
            "geohash is  qp097k\n",
            "geohash is  qp097m\n",
            "geohash is  qp097n\n",
            "geohash is  qp097p\n",
            "geohash is  qp097q\n",
            "updating negative prediction of  -0.0015553274390443068  for geohash  qp097q  to 0\n",
            "geohash is  qp097r\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp097s\n",
            "geohash is  qp097t\n",
            "geohash is  qp097u\n",
            "geohash is  qp097v\n",
            "geohash is  qp097w\n",
            "geohash is  qp097x\n",
            "updating negative prediction of  -0.0031744838  for geohash  qp097x  to 0\n",
            "updating negative prediction of  -0.0060898066  for geohash  qp097x  to 0\n",
            "updating negative prediction of  -0.0060898066  for geohash  qp097x  to 0\n",
            "updating negative prediction of  -0.00676924  for geohash  qp097x  to 0\n",
            "updating negative prediction of  -0.00082308054  for geohash  qp097x  to 0\n",
            "updating negative prediction of  -0.0009507537  for geohash  qp097x  to 0\n",
            "geohash is  qp097y\n",
            "geohash is  qp097z\n",
            "geohash is  qp0980\n",
            "geohash is  qp0981\n",
            "geohash is  qp0982\n",
            "geohash is  qp0983\n",
            "geohash is  qp0984\n",
            "geohash is  qp0985\n",
            "geohash is  qp0986\n",
            "geohash is  qp0987\n",
            "geohash is  qp0988\n",
            "geohash is  qp0989\n",
            "geohash is  qp098b\n",
            "geohash is  qp098c\n",
            "geohash is  qp098d\n",
            "geohash is  qp098e\n",
            "geohash is  qp098f\n",
            "geohash is  qp098g\n",
            "geohash is  qp098h\n",
            "geohash is  qp098j\n",
            "geohash is  qp098k\n",
            "geohash is  qp098m\n",
            "geohash is  qp098n\n",
            "geohash is  qp098p\n",
            "geohash is  qp098q\n",
            "geohash is  qp098r\n",
            "geohash is  qp098u\n",
            "updating negative prediction of  -6.7055225e-05  for geohash  qp098u  to 0\n",
            "updating negative prediction of  -0.002769117857215131  for geohash  qp098u  to 0\n",
            "geohash is  qp098v\n",
            "geohash is  qp0990\n",
            "geohash is  qp0991\n",
            "geohash is  qp0992\n",
            "geohash is  qp0993\n",
            "geohash is  qp0994\n",
            "geohash is  qp0995\n",
            "geohash is  qp0996\n",
            "geohash is  qp0997\n",
            "geohash is  qp0998\n",
            "geohash is  qp0999\n",
            "geohash is  qp099b\n",
            "geohash is  qp099c\n",
            "geohash is  qp099d\n",
            "geohash is  qp099e\n",
            "geohash is  qp099f\n",
            "updating negative prediction of  -4.965067e-05  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -4.965067e-05  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.0014760494  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.0017273426  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.00012920521663686428  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.00012920521663686428  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.00012920521663686428  for geohash  qp099f  to 0\n",
            "updating negative prediction of  -0.00012920521663686428  for geohash  qp099f  to 0\n",
            "geohash is  qp099g\n",
            "geohash is  qp099h\n",
            "geohash is  qp099j\n",
            "geohash is  qp099k\n",
            "geohash is  qp099m\n",
            "geohash is  qp099n\n",
            "geohash is  qp099p\n",
            "geohash is  qp099q\n",
            "geohash is  qp099r\n",
            "geohash is  qp099s\n",
            "geohash is  qp099t\n",
            "geohash is  qp099u\n",
            "geohash is  qp099v\n",
            "geohash is  qp099w\n",
            "geohash is  qp099x\n",
            "geohash is  qp099y\n",
            "geohash is  qp099z\n",
            "geohash is  qp09b0\n",
            "geohash is  qp09b1\n",
            "geohash is  qp09b2\n",
            "geohash is  qp09b3\n",
            "geohash is  qp09b4\n",
            "geohash is  qp09b5\n",
            "geohash is  qp09b6\n",
            "geohash is  qp09b7\n",
            "geohash is  qp09bd\n",
            "geohash is  qp09be\n",
            "geohash is  qp09bh\n",
            "geohash is  qp09bj\n",
            "geohash is  qp09bk\n",
            "geohash is  qp09bm\n",
            "geohash is  qp09bn\n",
            "geohash is  qp09bp\n",
            "geohash is  qp09bq\n",
            "geohash is  qp09br\n",
            "geohash is  qp09bt\n",
            "geohash is  qp09bv\n",
            "geohash is  qp09bw\n",
            "geohash is  qp09bx\n",
            "geohash is  qp09by\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09bz\n",
            "geohash is  qp09c2\n",
            "geohash is  qp09c6\n",
            "geohash is  qp09c7\n",
            "geohash is  qp09c8\n",
            "geohash is  qp09c9\n",
            "geohash is  qp09cb\n",
            "geohash is  qp09cc\n",
            "geohash is  qp09cd\n",
            "geohash is  qp09ce\n",
            "geohash is  qp09cf\n",
            "geohash is  qp09cg\n",
            "geohash is  qp09ch\n",
            "updating negative prediction of  -0.0002100240542460784  for geohash  qp09ch  to 0\n",
            "geohash is  qp09cj\n",
            "geohash is  qp09ck\n",
            "geohash is  qp09cm\n",
            "geohash is  qp09cn\n",
            "geohash is  qp09cp\n",
            "geohash is  qp09cq\n",
            "geohash is  qp09cr\n",
            "geohash is  qp09cs\n",
            "geohash is  qp09ct\n",
            "geohash is  qp09cu\n",
            "geohash is  qp09cv\n",
            "geohash is  qp09cw\n",
            "geohash is  qp09cx\n",
            "geohash is  qp09cy\n",
            "geohash is  qp09cz\n",
            "geohash is  qp09d0\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09d1\n",
            "geohash is  qp09d2\n",
            "geohash is  qp09d3\n",
            "geohash is  qp09d4\n",
            "geohash is  qp09d5\n",
            "geohash is  qp09d6\n",
            "geohash is  qp09d7\n",
            "geohash is  qp09d8\n",
            "geohash is  qp09d9\n",
            "geohash is  qp09db\n",
            "geohash is  qp09dc\n",
            "geohash is  qp09dd\n",
            "geohash is  qp09de\n",
            "geohash is  qp09df\n",
            "geohash is  qp09dg\n",
            "geohash is  qp09dh\n",
            "geohash is  qp09dj\n",
            "geohash is  qp09dk\n",
            "geohash is  qp09dm\n",
            "geohash is  qp09dn\n",
            "geohash is  qp09dp\n",
            "geohash is  qp09dq\n",
            "geohash is  qp09dr\n",
            "geohash is  qp09ds\n",
            "geohash is  qp09dt\n",
            "updating negative prediction of  -0.0049620867  for geohash  qp09dt  to 0\n",
            "updating negative prediction of  -0.007442713  for geohash  qp09dt  to 0\n",
            "updating negative prediction of  -0.00020217896  for geohash  qp09dt  to 0\n",
            "geohash is  qp09du\n",
            "updating negative prediction of  -0.02280873  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.028603017  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.015999317  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.0042659044  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.0043096542  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.00826484  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.0009497493665773113  for geohash  qp09du  to 0\n",
            "updating negative prediction of  -0.003164106928352317  for geohash  qp09du  to 0\n",
            "geohash is  qp09dv\n",
            "geohash is  qp09dw\n",
            "geohash is  qp09dx\n",
            "geohash is  qp09dy\n",
            "geohash is  qp09dz\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09e0\n",
            "updating negative prediction of  -0.00014388561  for geohash  qp09e0  to 0\n",
            "geohash is  qp09e1\n",
            "geohash is  qp09e2\n",
            "geohash is  qp09e3\n",
            "geohash is  qp09e4\n",
            "geohash is  qp09e5\n",
            "geohash is  qp09e6\n",
            "geohash is  qp09e7\n",
            "geohash is  qp09e8\n",
            "updating negative prediction of  -0.008746743  for geohash  qp09e8  to 0\n",
            "geohash is  qp09e9\n",
            "geohash is  qp09eb\n",
            "geohash is  qp09ec\n",
            "geohash is  qp09ed\n",
            "geohash is  qp09ee\n",
            "geohash is  qp09ef\n",
            "geohash is  qp09eg\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09eh\n",
            "geohash is  qp09ej\n",
            "geohash is  qp09ek\n",
            "geohash is  qp09em\n",
            "geohash is  qp09en\n",
            "geohash is  qp09ep\n",
            "geohash is  qp09eq\n",
            "geohash is  qp09er\n",
            "geohash is  qp09es\n",
            "geohash is  qp09et\n",
            "geohash is  qp09eu\n",
            "geohash is  qp09ev\n",
            "geohash is  qp09ew\n",
            "geohash is  qp09ex\n",
            "geohash is  qp09ey\n",
            "geohash is  qp09ez\n",
            "updating negative prediction of  -0.017022073  for geohash  qp09ez  to 0\n",
            "updating negative prediction of  -0.0006070137  for geohash  qp09ez  to 0\n",
            "geohash is  qp09f0\n",
            "geohash is  qp09f1\n",
            "geohash is  qp09f2\n",
            "geohash is  qp09f3\n",
            "geohash is  qp09f4\n",
            "geohash is  qp09f5\n",
            "geohash is  qp09f6\n",
            "geohash is  qp09f7\n",
            "geohash is  qp09f8\n",
            "geohash is  qp09f9\n",
            "geohash is  qp09fb\n",
            "geohash is  qp09fc\n",
            "geohash is  qp09fd\n",
            "geohash is  qp09fe\n",
            "geohash is  qp09ff\n",
            "geohash is  qp09fg\n",
            "geohash is  qp09fh\n",
            "geohash is  qp09fj\n",
            "geohash is  qp09fk\n",
            "geohash is  qp09fm\n",
            "geohash is  qp09fq\n",
            "geohash is  qp09fr\n",
            "geohash is  qp09fs\n",
            "geohash is  qp09ft\n",
            "geohash is  qp09fu\n",
            "geohash is  qp09fv\n",
            "geohash is  qp09fw\n",
            "geohash is  qp09fx\n",
            "geohash is  qp09fy\n",
            "geohash is  qp09fz\n",
            "geohash is  qp09g0\n",
            "geohash is  qp09g1\n",
            "geohash is  qp09g2\n",
            "geohash is  qp09g3\n",
            "geohash is  qp09g4\n",
            "geohash is  qp09g5\n",
            "geohash is  qp09g6\n",
            "geohash is  qp09g7\n",
            "geohash is  qp09g8\n",
            "geohash is  qp09g9\n",
            "geohash is  qp09gb\n",
            "geohash is  qp09gc\n",
            "geohash is  qp09gd\n",
            "geohash is  qp09ge\n",
            "geohash is  qp09gf\n",
            "geohash is  qp09gg\n",
            "geohash is  qp09gh\n",
            "geohash is  qp09gj\n",
            "geohash is  qp09gk\n",
            "geohash is  qp09gm\n",
            "geohash is  qp09gn\n",
            "geohash is  qp09gp\n",
            "geohash is  qp09gq\n",
            "geohash is  qp09gr\n",
            "geohash is  qp09gs\n",
            "geohash is  qp09gt\n",
            "geohash is  qp09gu\n",
            "geohash is  qp09gv\n",
            "geohash is  qp09gw\n",
            "geohash is  qp09gx\n",
            "geohash is  qp09gy\n",
            "geohash is  qp09gz\n",
            "geohash is  qp09h0\n",
            "geohash is  qp09h1\n",
            "geohash is  qp09h3\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09h4\n",
            "updating negative prediction of  -0.00235100056477617  for geohash  qp09h4  to 0\n",
            "updating negative prediction of  -0.0016657333956846833  for geohash  qp09h4  to 0\n",
            "geohash is  qp09h5\n",
            "geohash is  qp09h6\n",
            "updating negative prediction of  -2.7298927e-05  for geohash  qp09h6  to 0\n",
            "geohash is  qp09h7\n",
            "geohash is  qp09he\n",
            "geohash is  qp09hh\n",
            "geohash is  qp09hj\n",
            "geohash is  qp09hk\n",
            "geohash is  qp09hm\n",
            "geohash is  qp09hn\n",
            "geohash is  qp09hp\n",
            "geohash is  qp09hq\n",
            "geohash is  qp09hr\n",
            "geohash is  qp09hs\n",
            "geohash is  qp09ht\n",
            "updating negative prediction of  -0.0019897223  for geohash  qp09ht  to 0\n",
            "geohash is  qp09hv\n",
            "geohash is  qp09hw\n",
            "geohash is  qp09hx\n",
            "geohash is  qp09hy\n",
            "geohash is  qp09hz\n",
            "geohash is  qp09j5\n",
            "updating negative prediction of  -0.00023967028  for geohash  qp09j5  to 0\n",
            "updating negative prediction of  -0.00056546926  for geohash  qp09j5  to 0\n",
            "geohash is  qp09j7\n",
            "geohash is  qp09j9\n",
            "geohash is  qp09jb\n",
            "geohash is  qp09jc\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09jd\n",
            "geohash is  qp09je\n",
            "geohash is  qp09jf\n",
            "geohash is  qp09jg\n",
            "geohash is  qp09jj\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09jk\n",
            "geohash is  qp09jm\n",
            "geohash is  qp09jn\n",
            "geohash is  qp09jp\n",
            "geohash is  qp09jq\n",
            "geohash is  qp09jr\n",
            "updating negative prediction of  -0.00033843517  for geohash  qp09jr  to 0\n",
            "updating negative prediction of  -0.0007273555  for geohash  qp09jr  to 0\n",
            "geohash is  qp09js\n",
            "geohash is  qp09jt\n",
            "geohash is  qp09ju\n",
            "geohash is  qp09jv\n",
            "geohash is  qp09jw\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09jx\n",
            "geohash is  qp09jy\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09jz\n",
            "geohash is  qp09k0\n",
            "geohash is  qp09k1\n",
            "geohash is  qp09k2\n",
            "geohash is  qp09k3\n",
            "geohash is  qp09k4\n",
            "geohash is  qp09k5\n",
            "geohash is  qp09k6\n",
            "geohash is  qp09k7\n",
            "geohash is  qp09k8\n",
            "geohash is  qp09k9\n",
            "geohash is  qp09kb\n",
            "geohash is  qp09kc\n",
            "geohash is  qp09kd\n",
            "geohash is  qp09ke\n",
            "geohash is  qp09kf\n",
            "geohash is  qp09kg\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09kj\n",
            "geohash is  qp09km\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09kn\n",
            "geohash is  qp09kq\n",
            "geohash is  qp09kr\n",
            "geohash is  qp09ks\n",
            "geohash is  qp09kt\n",
            "geohash is  qp09ku\n",
            "geohash is  qp09kv\n",
            "geohash is  qp09kw\n",
            "geohash is  qp09kx\n",
            "geohash is  qp09ky\n",
            "geohash is  qp09kz\n",
            "geohash is  qp09m0\n",
            "geohash is  qp09m1\n",
            "geohash is  qp09m2\n",
            "geohash is  qp09m3\n",
            "geohash is  qp09m6\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09m7\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09m8\n",
            "geohash is  qp09m9\n",
            "geohash is  qp09mb\n",
            "updating negative prediction of  -0.0020228592948231734  for geohash  qp09mb  to 0\n",
            "geohash is  qp09mc\n",
            "updating negative prediction of  -0.002761299060538216  for geohash  qp09mc  to 0\n",
            "geohash is  qp09md\n",
            "geohash is  qp09me\n",
            "geohash is  qp09mf\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09mj\n",
            "geohash is  qp09mm\n",
            "geohash is  qp09mn\n",
            "geohash is  qp09mp\n",
            "geohash is  qp09mq\n",
            "geohash is  qp09mr\n",
            "geohash is  qp09ms\n",
            "geohash is  qp09mt\n",
            "geohash is  qp09mu\n",
            "geohash is  qp09mv\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09mw\n",
            "geohash is  qp09mx\n",
            "geohash is  qp09my\n",
            "geohash is  qp09mz\n",
            "geohash is  qp09n0\n",
            "geohash is  qp09n1\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09n4\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09n5\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09nj\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09np\n",
            "geohash is  qp09q0\n",
            "geohash is  qp09q1\n",
            "geohash is  qp09q4\n",
            "geohash is  qp09q5\n",
            "geohash is  qp09qh\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09qj\n",
            "geohash is  qp09qn\n",
            "geohash is  qp09qp\n",
            "geohash is  qp09s0\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09s1\n",
            "geohash is  qp09s2\n",
            "geohash is  qp09s3\n",
            "geohash is  qp09s4\n",
            "geohash is  qp09s5\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09s6\n",
            "geohash is  qp09s7\n",
            "geohash is  qp09s8\n",
            "geohash is  qp09s9\n",
            "geohash is  qp09sb\n",
            "geohash is  qp09sc\n",
            "geohash is  qp09sd\n",
            "geohash is  qp09se\n",
            "geohash is  qp09sf\n",
            "geohash is  qp09sg\n",
            "geohash is  qp09sh\n",
            "geohash is  qp09sj\n",
            "geohash is  qp09sk\n",
            "geohash is  qp09sm\n",
            "geohash is  qp09sn\n",
            "geohash is  qp09sp\n",
            "geohash is  qp09sq\n",
            "geohash is  qp09sr\n",
            "geohash is  qp09ss\n",
            "geohash is  qp09st\n",
            "geohash is  qp09su\n",
            "geohash is  qp09sv\n",
            "geohash is  qp09sw\n",
            "geohash is  qp09sx\n",
            "geohash is  qp09sy\n",
            "geohash is  qp09sz\n",
            "geohash is  qp09t0\n",
            "geohash is  qp09t1\n",
            "updating negative prediction of  -4.150016905079556e-05  for geohash  qp09t1  to 0\n",
            "geohash is  qp09t2\n",
            "geohash is  qp09t3\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09t4\n",
            "geohash is  qp09t5\n",
            "geohash is  qp09t7\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09t8\n",
            "geohash is  qp09t9\n",
            "geohash is  qp09tb\n",
            "geohash is  qp09te\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09tg\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09th\n",
            "geohash is  qp09tj\n",
            "geohash is  qp09tk\n",
            "geohash is  qp09tm\n",
            "geohash is  qp09tn\n",
            "geohash is  qp09tp\n",
            "geohash is  qp09tq\n",
            "geohash is  qp09tr\n",
            "geohash is  qp09ts\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09tt\n",
            "geohash is  qp09tu\n",
            "geohash is  qp09tv\n",
            "geohash is  qp09tw\n",
            "geohash is  qp09tx\n",
            "geohash is  qp09ty\n",
            "geohash is  qp09u0\n",
            "geohash is  qp09u1\n",
            "geohash is  qp09u2\n",
            "geohash is  qp09u3\n",
            "updating negative prediction of  -0.0022114515  for geohash  qp09u3  to 0\n",
            "updating negative prediction of  -0.0022528172  for geohash  qp09u3  to 0\n",
            "geohash is  qp09u4\n",
            "geohash is  qp09u5\n",
            "geohash is  qp09u6\n",
            "geohash is  qp09u7\n",
            "updating negative prediction of  -0.008640766  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.008640766  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.008640766  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.008640766  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.008640766  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.0044379234  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.0024148226  for geohash  qp09u7  to 0\n",
            "updating negative prediction of  -0.004573345  for geohash  qp09u7  to 0\n",
            "geohash is  qp09u8\n",
            "geohash is  qp09u9\n",
            "geohash is  qp09ub\n",
            "updating negative prediction of  -0.0003145155428222097  for geohash  qp09ub  to 0\n",
            "updating negative prediction of  -0.00040005230562760636  for geohash  qp09ub  to 0\n",
            "geohash is  qp09uc\n",
            "geohash is  qp09ud\n",
            "geohash is  qp09ue\n",
            "geohash is  qp09uf\n",
            "geohash is  qp09ug\n",
            "geohash is  qp09uh\n",
            "updating negative prediction of  -0.000582612567004189  for geohash  qp09uh  to 0\n",
            "geohash is  qp09uj\n",
            "geohash is  qp09uk\n",
            "geohash is  qp09um\n",
            "geohash is  qp09un\n",
            "geohash is  qp09up\n",
            "geohash is  qp09uq\n",
            "geohash is  qp09ur\n",
            "geohash is  qp09us\n",
            "geohash is  qp09ut\n",
            "geohash is  qp09uu\n",
            "geohash is  qp09uv\n",
            "geohash is  qp09uw\n",
            "geohash is  qp09ux\n",
            "geohash is  qp09uy\n",
            "geohash is  qp09uz\n",
            "geohash is  qp09v0\n",
            "geohash is  qp09v1\n",
            "geohash is  qp09v2\n",
            "geohash is  qp09v3\n",
            "geohash is  qp09v4\n",
            "updating negative prediction of  -0.00044447184  for geohash  qp09v4  to 0\n",
            "geohash is  qp09v5\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09v6\n",
            "geohash is  qp09v7\n",
            "geohash is  qp09v8\n",
            "updating negative prediction of  -0.00040364265  for geohash  qp09v8  to 0\n",
            "geohash is  qp09v9\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09vb\n",
            "geohash is  qp09vc\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09vd\n",
            "geohash is  qp09ve\n",
            "updating negative prediction of  -4.476309e-05  for geohash  qp09ve  to 0\n",
            "updating negative prediction of  -0.002490759  for geohash  qp09ve  to 0\n",
            "updating negative prediction of  -0.0028238297  for geohash  qp09ve  to 0\n",
            "geohash is  qp09vf\n",
            "geohash is  qp09vg\n",
            "geohash is  qp09vh\n",
            "geohash is  qp09vj\n",
            "geohash is  qp09vk\n",
            "geohash is  qp09vm\n",
            "geohash is  qp09vn\n",
            "geohash is  qp09vp\n",
            "geohash is  qp09vq\n",
            "geohash is  qp09vr\n",
            "geohash is  qp09vs\n",
            "geohash is  qp09vt\n",
            "geohash is  qp09vu\n",
            "geohash is  qp09vv\n",
            "geohash is  qp09vw\n",
            "geohash is  qp09vx\n",
            "geohash is  qp09vy\n",
            "geohash is  qp09vz\n",
            "updating negative prediction of  -0.0012870431  for geohash  qp09vz  to 0\n",
            "updating negative prediction of  -0.0014784932  for geohash  qp09vz  to 0\n",
            "geohash is  qp09w0\n",
            "geohash is  qp09w1\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09w4\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp09w5\n",
            "updating negative prediction of  -0.0081656575  for geohash  qp09w5  to 0\n",
            "geohash is  qp09wh\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09wn\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp09wp\n",
            "updating negative prediction of  -0.0010181069  for geohash  qp09wp  to 0\n",
            "geohash is  qp09y0\n",
            "geohash is  qp09y1\n",
            "geohash is  qp09y4\n",
            "geohash is  qp09y5\n",
            "geohash is  qp09yh\n",
            "geohash is  qp09yj\n",
            "geohash is  qp09yn\n",
            "geohash is  qp09yp\n",
            "geohash is  qp0d00\n",
            "geohash is  qp0d01\n",
            "geohash is  qp0d02\n",
            "geohash is  qp0d03\n",
            "geohash is  qp0d04\n",
            "geohash is  qp0d05\n",
            "updating negative prediction of  -4.285917472878613e-06  for geohash  qp0d05  to 0\n",
            "geohash is  qp0d06\n",
            "geohash is  qp0d07\n",
            "updating negative prediction of  -0.00011587143  for geohash  qp0d07  to 0\n",
            "updating negative prediction of  -0.0012603796578692435  for geohash  qp0d07  to 0\n",
            "geohash is  qp0d08\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d0b\n",
            "geohash is  qp0d0c\n",
            "geohash is  qp0d0f\n",
            "geohash is  qp0d0g\n",
            "geohash is  qp0d0h\n",
            "geohash is  qp0d0j\n",
            "geohash is  qp0d0k\n",
            "geohash is  qp0d0m\n",
            "geohash is  qp0d0n\n",
            "geohash is  qp0d0q\n",
            "geohash is  qp0d0s\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d0t\n",
            "updating negative prediction of  -0.002520739555251187  for geohash  qp0d0t  to 0\n",
            "geohash is  qp0d0u\n",
            "updating negative prediction of  -0.0009290558086808146  for geohash  qp0d0u  to 0\n",
            "geohash is  qp0d0v\n",
            "geohash is  qp0d0w\n",
            "geohash is  qp0d0y\n",
            "geohash is  qp0d10\n",
            "geohash is  qp0d11\n",
            "geohash is  qp0d12\n",
            "geohash is  qp0d13\n",
            "geohash is  qp0d14\n",
            "geohash is  qp0d15\n",
            "geohash is  qp0d16\n",
            "geohash is  qp0d17\n",
            "geohash is  qp0d18\n",
            "geohash is  qp0d19\n",
            "geohash is  qp0d1b\n",
            "geohash is  qp0d1c\n",
            "geohash is  qp0d1d\n",
            "geohash is  qp0d1e\n",
            "geohash is  qp0d1f\n",
            "geohash is  qp0d1g\n",
            "geohash is  qp0d1h\n",
            "geohash is  qp0d1j\n",
            "geohash is  qp0d1k\n",
            "geohash is  qp0d1m\n",
            "geohash is  qp0d1n\n",
            "geohash is  qp0d1q\n",
            "geohash is  qp0d1s\n",
            "geohash is  qp0d1t\n",
            "geohash is  qp0d1u\n",
            "geohash is  qp0d1v\n",
            "geohash is  qp0d1w\n",
            "geohash is  qp0d1y\n",
            "geohash is  qp0d42\n",
            "updating negative prediction of  -0.00021457672  for geohash  qp0d42  to 0\n",
            "updating negative prediction of  -0.00026142597  for geohash  qp0d42  to 0\n",
            "updating negative prediction of  -0.0008871460855471527  for geohash  qp0d42  to 0\n",
            "geohash is  qp0d43\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d44\n",
            "geohash is  qp0d45\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp0d48\n",
            "geohash is  qp0d49\n",
            "updating negative prediction of  -0.020566344  for geohash  qp0d49  to 0\n",
            "updating negative prediction of  -0.03365451  for geohash  qp0d49  to 0\n",
            "updating negative prediction of  -0.028271317  for geohash  qp0d49  to 0\n",
            "updating negative prediction of  -0.03874594  for geohash  qp0d49  to 0\n",
            "updating negative prediction of  -0.03818029  for geohash  qp0d49  to 0\n",
            "geohash is  qp0d4b\n",
            "geohash is  qp0d4c\n",
            "geohash is  qp0d4d\n",
            "updating negative prediction of  -0.0030323863  for geohash  qp0d4d  to 0\n",
            "updating negative prediction of  -0.008905776308571377  for geohash  qp0d4d  to 0\n",
            "geohash is  qp0d4e\n",
            "geohash is  qp0d4f\n",
            "geohash is  qp0d4g\n",
            "geohash is  qp0d4h\n",
            "geohash is  qp0d4j\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d4m\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp0d4n\n",
            "geohash is  qp0d4q\n",
            "geohash is  qp0d4s\n",
            "geohash is  qp0d4t\n",
            "geohash is  qp0d4u\n",
            "geohash is  qp0d4v\n",
            "geohash is  qp0d4w\n",
            "geohash is  qp0d4y\n",
            "geohash is  qp0d50\n",
            "geohash is  qp0d51\n",
            "geohash is  qp0d52\n",
            "geohash is  qp0d53\n",
            "geohash is  qp0d54\n",
            "geohash is  qp0d55\n",
            "updating negative prediction of  -0.0028057098  for geohash  qp0d55  to 0\n",
            "updating negative prediction of  -0.0037819743  for geohash  qp0d55  to 0\n",
            "updating negative prediction of  -0.008027792  for geohash  qp0d55  to 0\n",
            "geohash is  qp0d56\n",
            "geohash is  qp0d57\n",
            "geohash is  qp0d58\n",
            "geohash is  qp0d59\n",
            "geohash is  qp0d5b\n",
            "geohash is  qp0d5c\n",
            "geohash is  qp0d5d\n",
            "updating negative prediction of  -3.695488e-06  for geohash  qp0d5d  to 0\n",
            "updating negative prediction of  -3.695488e-06  for geohash  qp0d5d  to 0\n",
            "updating negative prediction of  -0.00040669803220960156  for geohash  qp0d5d  to 0\n",
            "geohash is  qp0d5e\n",
            "geohash is  qp0d5f\n",
            "geohash is  qp0d5g\n",
            "geohash is  qp0d5h\n",
            "geohash is  qp0d5j\n",
            "geohash is  qp0d5k\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp0d5m\n",
            "geohash is  qp0d5n\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d5q\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d5s\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp0d5t\n",
            "geohash is  qp0d5u\n",
            "geohash is  qp0dh0\n",
            "geohash is  qp0dh1\n",
            "geohash is  qp0dh2\n",
            "updating negative prediction of  -0.00071901083  for geohash  qp0dh2  to 0\n",
            "updating negative prediction of  -0.0028908253  for geohash  qp0dh2  to 0\n",
            "geohash is  qp0dh3\n",
            "geohash is  qp0dh4\n",
            "geohash is  qp0dh5\n",
            "geohash is  qp0dh6\n",
            "geohash is  qp0dh7\n",
            "geohash is  qp0dh8\n",
            "geohash is  qp0dh9\n",
            "geohash is  qp0dhb\n",
            "geohash is  qp0dhc\n",
            "geohash is  qp0dhd\n",
            "geohash is  qp0dhe\n",
            "geohash is  qp0dhf\n",
            "geohash is  qp0dhg\n",
            "geohash is  qp0dhh\n",
            "updating negative prediction of  -0.0013706684  for geohash  qp0dhh  to 0\n",
            "geohash is  qp0dhj\n",
            "geohash is  qp0dhk\n",
            "geohash is  qp0dhm\n",
            "geohash is  qp0dhq\n",
            "updating negative prediction of  -0.0009174347  for geohash  qp0dhq  to 0\n",
            "updating negative prediction of  -0.0013425946  for geohash  qp0dhq  to 0\n",
            "updating negative prediction of  -0.0033316612  for geohash  qp0dhq  to 0\n",
            "geohash is  qp0dhs\n",
            "geohash is  qp0dht\n",
            "geohash is  qp0dhu\n",
            "geohash is  qp0dhv\n",
            "geohash is  qp0dhw\n",
            "geohash is  qp0dhy\n",
            "geohash is  qp0dj0\n",
            "geohash is  qp0dj1\n",
            "geohash is  qp0dj2\n",
            "geohash is  qp0dj3\n",
            "geohash is  qp0dj4\n",
            "geohash is  qp0dj5\n",
            "geohash is  qp0dj6\n",
            "updating negative prediction of  -0.0010989904  for geohash  qp0dj6  to 0\n",
            "updating negative prediction of  -0.0028633475  for geohash  qp0dj6  to 0\n",
            "updating negative prediction of  -0.0008432865  for geohash  qp0dj6  to 0\n",
            "geohash is  qp0dj7\n",
            "geohash is  qp0dj8\n",
            "geohash is  qp0dj9\n",
            "geohash is  qp0djb\n",
            "geohash is  qp0djc\n",
            "geohash is  qp0djd\n",
            "geohash is  qp0dje\n",
            "geohash is  qp0djf\n",
            "geohash is  qp0djg\n",
            "geohash is  qp0djh\n",
            "geohash is  qp0djj\n",
            "geohash is  qp0djk\n",
            "geohash is  qp0djm\n",
            "geohash is  qp0djn\n",
            "geohash is  qp0djq\n",
            "geohash is  qp0djs\n",
            "geohash is  qp0djt\n",
            "geohash is  qp0dju\n",
            "geohash is  qp0djv\n",
            "updating negative prediction of  -0.0027388066109395454  for geohash  qp0djv  to 0\n",
            "geohash is  qp0djw\n",
            "geohash is  qp0djy\n",
            "geohash is  qp0dn0\n",
            "geohash is  qp0dn1\n",
            "geohash is  qp0dn4\n",
            "geohash is  qp0dn5\n",
            "geohash is  qp0dnh\n",
            "geohash is  qp0dnj\n",
            "updating negative prediction of  -0.00015491247  for geohash  qp0dnj  to 0\n",
            "geohash is  qp0dnn\n",
            "0.02766  rsme for model  predictedXGBoost1 . Max RMSE is at index 571 with  0.34754 ,Min RMSE is at index 1185 with  0.00109\n",
            "0.01997  rsme for model  predictedXGBoost2 . Max RMSE is at index 571 with  0.15517 ,Min RMSE is at index 336 with  0.00078\n",
            "0.01933  rsme for model  gradientBoostRegressor1 . Max RMSE is at index 64 with  0.11832 ,Min RMSE is at index 185 with  0.00092\n",
            "0.01919  rsme for model  gradientBoostRegressor4 . Max RMSE is at index 46 with  0.17109 ,Min RMSE is at index 1185 with  0.00109\n",
            "0.03059  rsme for model  gradientBoostRegressor6 . Max RMSE is at index 791 with  0.3949 ,Min RMSE is at index 1185 with  0.00109\n",
            "0.01915  rsme for model  gradientBoostRegressor7 . Max RMSE is at index 46 with  0.17073 ,Min RMSE is at index 1185 with  0.00109\n",
            "0.01856  rsme for model  ensembleXGBoost2Gradient4 . Max RMSE is at index 571 with  0.12232 ,Min RMSE is at index 1185 with  0.00109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZKFdukWLuT",
        "colab_type": "text"
      },
      "source": [
        "# GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBQerj_T3dI",
        "colab_type": "code",
        "outputId": "0d65f277-9872-4ca2-f8f3-4cb9065bb128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "for pred in trainedTests:\n",
        "  trainedpreds[pred]={}\n",
        "\n",
        "XGparams = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.3,0.6, 0.8, 1.0],\n",
        "        'max_depth': [1, 3, 4, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'alpha': [0, 10]\n",
        "        }\n",
        "  \n",
        "#See more at: https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.Bwu6NdCJ.dpuf\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,n_estimators=40,max_depth=1,min_samples_split=500, min_samples_leaf=50, subsample=0.8, random_state=10),\n",
        "gsearch1 = RandomizedSearchCV(estimator = xgb.XGBRegressor(n_estimators = 1000,silent=True),\n",
        "  scoring='neg_mean_squared_error', \n",
        "  param_distributions  = XGparams, n_jobs=4,iid=False, cv=5)\n",
        "\n",
        "\n",
        "for randGeoHash in randGeoHashes:  \n",
        "  randDataSet=dataset[dataset.geohash6==randGeoHash]\n",
        "  #print(randGeoHash)\n",
        "  testData=randDataSet[-5:]\n",
        "  trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]  \n",
        "  trainData1,trainLabel1,testData1=splitData1(dataset[dataset.geohash6==randGeoHash], dataset) \n",
        "  \n",
        "  if(len(randDataSet) <= 5 or len(trainData) <= 1 or  len(trainData1) <= 5):\n",
        "    result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    for pred in trainedpreds:\n",
        "      trainedpreds[pred][randGeoHash]=result\n",
        "    continue  \n",
        "      \n",
        "  gsearch1.fit(trainData1,trainLabel1)\n",
        "\n",
        "gsearch1.best_params_, gsearch1.best_score_\n",
        "\n",
        "#\n",
        "#({'alpha': 0,\n",
        "#  'colsample_bytree': 0.3,\n",
        "#  'gamma': 0.5,\n",
        "#  'learning_rate': 0.1,\n",
        "#  'max_depth': 7,\n",
        "#  'min_child_weight': 10,\n",
        "#  'subsample': 1.0},\n",
        "#3 -0.00021086111001349567)\n"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-222-f886f1bae512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6cycBk59S7",
        "colab_type": "code",
        "outputId": "c6e98b5c-c2fa-426d-e89d-924c0d209e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# final best\n",
        "#0.02766  rsme for model  predictedXGBoost1 . Max RMSE is at index 571 with  0.34754 ,Min RMSE is at index 1185 with  0.00109\n",
        "#0.01997  rsme for model  predictedXGBoost2 . Max RMSE is at index 571 with  0.15517 ,Min RMSE is at index 336 with  0.00078\n",
        "#0.01936  rsme for model  gradientBoostRegressor1 . Max RMSE is at index 1325 with  0.12165 ,Min RMSE is at index 185 with  0.00091\n",
        "#0.01918  rsme for model  gradientBoostRegressor4 . Max RMSE is at index 46 with  0.18411 ,Min RMSE is at index 1185 with  0.00109\n",
        "#0.03053  rsme for model  gradientBoostRegressor6 . Max RMSE is at index 791 with  0.38595 ,Min RMSE is at index 1185 with  0.00109\n",
        "#0.02798  rsme for model  gradientBoostRegressor7 . Max RMSE is at index 46 with  0.37887 ,Min RMSE is at index 1022 with  0.00102\n",
        "\n",
        "# worse for g4 is qp03mb \n",
        "hashToView=randGeoHashes[1325]\n",
        "\n",
        "aa=testDatas[hashToView]\n",
        "aa['x1']=preds['predictedXGBoost1'][hashToView]\n",
        "aa['x2']=preds['predictedXGBoost2'][hashToView]\n",
        "aa['g1']=preds['gradientBoostRegressor1'][hashToView]\n",
        "aa['g4']=preds['gradientBoostRegressor4'][hashToView]\n",
        "#aa\n",
        "print(rsme(aa['x1'],aa['demand'],'x1'))\n",
        "print(rsme(aa['x2'],aa['demand'],'x2'))\n",
        "print(rsme(aa['g1'],aa['demand'],'g1'))\n",
        "print(rsme(aa['g4'],aa['demand'],'g4'))\n",
        "print(hashToView)\n",
        "trainData=dataset[dataset.geohash6==hashToView]\n",
        "print(len(trainData),np.mean(trainData.demand))\n",
        "trainData=dataset[dataset.geohash6==randGeoHashes[1325]]\n",
        "print(len(trainData),np.mean(trainData.demand))\n",
        "trainData=dataset[dataset.geohash6==randGeoHashes[1185]]\n",
        "print(len(trainData),np.mean(trainData.demand))\n",
        "trainData=dataset[dataset.geohash6==randGeoHashes[571]]\n",
        "print(len(trainData),np.mean(trainData.demand))\n",
        "#len(trainData6)-len(trainData5)\n",
        "#dataset.groupby('geohash6').demand.count()\n",
        "\n"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.011902416455830683\n",
            "0.016191612259280987\n",
            "0.01699688043301202\n",
            "0.022841627702544616\n",
            "qp0dj5\n",
            "4198 0.050195420434907254\n",
            "334 0.011068336163433478\n",
            "85 0.0031129921541529566\n",
            "5839 0.35184206933551027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttwmHYHvkD0b",
        "colab_type": "code",
        "outputId": "925c4c73-4ef1-42b1-c437-3d1a34ad04bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=30)\n",
        "X=trainData4.values\n",
        "#X=X.drop(['datetime','timestamp','geohash6'],axis=1).values\n",
        "testX=testData4.values\n",
        "#testX=testX.drop(['datetime','timestamp','geohash6'],axis=1).values\n",
        "\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(trainX)\n",
        "#plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
        "len(y_kmeans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 16, 16, 16, 16], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU4Tut8PkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Evh4Ug4ill8",
        "colab_type": "code",
        "outputId": "28599631-f881-469b-d5fa-f83c5fe1da2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!pip install catboost "
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8a/a867c35770291646b085e9248814eb32dbe2aa824715b08e40cd92d0a83e/catboost-0.15.1-cp36-none-manylinux1_x86_64.whl (61.0MB)\n",
            "\u001b[K     || 61.1MB 613kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.4)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMLVGnG5TGlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#trainDataX\n",
        "\n",
        "#display(SVG(graph.pipe(format='svg')))\n",
        "#display.display(SVG(str_tree)\n",
        "#trainData1,trainLabel1,testData1=splitData1(randDataSet, dataset)\n",
        "#trainData2,trainLabel2,testData2=splitData2(randDataSet, dataset)\n",
        "#trainData3,trainLabel3,testData3=splitData3(randDataSet, dataset)\n",
        "#trainData4,trainLabel4,testData4=splitData4(randDataSet, dataset)\n",
        "\n",
        "#for row in randDataSet.itertuples():\n",
        "#randDataSet['neighborPrevDemand']=dataset.loc[randDataSet['geohash6']].mean()\n",
        "#print('length of 1st:',len(trainData1)+len(testData1),',2th:',len(trainData2)+len(testData2),',3th:',len(trainData3)+len(testData3),' 4th ',len(trainData4)+len(testData4))\n",
        "#trainData4\n",
        "#preds\n",
        "# TODO write method to find geohash with highest error and see why, how to improve\n",
        "\n",
        "#model_fit = arima_model.fit(trainData.demand, suppress_warnings=True)\n",
        "#trainData.demand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7A7bH2nfuX0",
        "colab_type": "code",
        "outputId": "adc7e7da-8fd1-4f59-f2a4-420ec52b9277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06192209, 0.05783723, 0.0600192 , 0.05783723, 0.06192209])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVnRC-g9_DMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictedRandForestRegressor1['qp03zx']\n",
        "randGeoHash='qp02yu'\n",
        "randDataSet=dataset.loc[dataset.geohash6==randGeoHash]\n",
        "\n",
        "testData=randDataSet[-5:]\n",
        "trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]\n",
        "#trainData.plot(x='x',y='demand')\n",
        "#testData.plot(x='x',y='demand')\n",
        "\n",
        "#predictInvalidPreds('qp08fu',predictedArima2)\n",
        "#predictedArima2\n",
        "#np.isfinite(predictedArima1['qp03zx'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ8o9wI_-b-i",
        "colab_type": "text"
      },
      "source": [
        "#HeatMap with time to see if any ideas to improve model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bQlAXTfx3XQ-",
        "colab": {}
      },
      "source": [
        "#https://alysivji.github.io/getting-started-with-folium.html\n",
        "# max value seem like 999 even though manual say 0-1\n",
        "mapData=[]\n",
        "#time_in_x_slider = 7 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "for x in range(5850,dataset.x.max()): # max seem like 30\n",
        "  #range(0, dataset.x.max())\n",
        "  # skip so that faster but will missed some logic\n",
        "  #if(x % 500 != 0):\n",
        "  #   continue\n",
        "  subData=[]\n",
        "  for row in dataset.loc[dataset['x']==x].itertuples():\n",
        "    gh6decoded=geohash.decode(getattr(row,'geohash6'))      \n",
        "    subData.append([gh6decoded[0],gh6decoded[1],getattr(row,'demand')*10])\n",
        "  mapData.append(subData)\n",
        "        \n",
        "m=folium.Map([mapData[0][0][0],mapData[0][0][1]], zoom_start=10)\n",
        "m.add_child(plugins.HeatMapWithTime(mapData, radius=10, auto_play=True)) # seem like cannot change speed or loop\n",
        "m.add_child(folium.LatLngPopup())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odK7-7hv-fGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "72069bfe-f786-4c0b-b89e-b812c2e77c79"
      },
      "source": [
        "#https://stackoverflow.com/questions/3810865/matplotlib-unknown-projection-3d-error\n",
        "visData1=dataset[dataset.geohash6==np.random.choice(dataset.geohash6.unique(),2)[0]]\n",
        "trainDataa,trainLabela,testDataa=splitData10(visData1, dataset)\n",
        "visData2=dataset[dataset.geohash6==np.random.choice(dataset.geohash6.unique(),2)[0]]\n",
        "trainDatab,trainLabelb,testDatab=splitData10(visData2, dataset)\n",
        "trainDataa,trainLabela,testDataa=splitData10(dataset[dataset.geohash6=='qp0d4m'],dataset)\n",
        "\n",
        "#a=pd.DataFrame()\n",
        "#testDataa\n",
        "#trainDataa['demand']=trainLabel6a\n",
        "#trainDatab['demand']=trainLabel6b\n",
        "#a['6a']=trainDataa.groupby('timestampX').describe()\n",
        "#a['6b']=trainDatab.groupby('timestampX').describe()\n",
        "\n",
        "\n",
        "#a\n",
        "#randGeoHashes=\n",
        "#fig,ax1 = plt.subplots(figsize=(4,4))\n",
        "#ax1.scatter(trainData6a['timestampX'], trainLabel6a)\n",
        "#fig,ax2 = plt.subplots(figsize=(4,4))\n",
        "#ax2.scatter(trainData6b['timestampX'], trainLabel6b)\n",
        "#plt.show()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>timestampX</th>\n",
              "      <th>dayOfWeek</th>\n",
              "      <th>x</th>\n",
              "      <th>demandEmptyTimeCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3692545</th>\n",
              "      <td>42</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>3985</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         day  timestampX  dayOfWeek     x  demandEmptyTimeCount\n",
              "3692545   42          49          0  3985                   0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS6y_biDNEVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        },
        "outputId": "15f5a16a-2193-434e-ab6a-1870e1ac92ef"
      },
      "source": [
        "\n",
        "  "
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>timestampX</th>\n",
              "      <th>dayOfWeek</th>\n",
              "      <th>x</th>\n",
              "      <th>demand</th>\n",
              "      <th>demandEmptyTimeCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1052018</th>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4416</td>\n",
              "      <td>0.094914</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1947725</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4417</td>\n",
              "      <td>0.070080</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419870</th>\n",
              "      <td>47.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4418</td>\n",
              "      <td>0.097505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3860768</th>\n",
              "      <td>47.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4419</td>\n",
              "      <td>0.105218</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480279</th>\n",
              "      <td>47.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4420</td>\n",
              "      <td>0.135887</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3590441</th>\n",
              "      <td>47.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4421</td>\n",
              "      <td>0.088575</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966622</th>\n",
              "      <td>47.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4422</td>\n",
              "      <td>0.133793</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2269011</th>\n",
              "      <td>47.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4423</td>\n",
              "      <td>0.132499</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417416</th>\n",
              "      <td>47.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4424</td>\n",
              "      <td>0.156030</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403063</th>\n",
              "      <td>47.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4425</td>\n",
              "      <td>0.139918</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3411649</th>\n",
              "      <td>47.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4426</td>\n",
              "      <td>0.120847</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1787719</th>\n",
              "      <td>47.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4427</td>\n",
              "      <td>0.110508</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413402</th>\n",
              "      <td>47.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4428</td>\n",
              "      <td>0.150303</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1888738</th>\n",
              "      <td>47.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4429</td>\n",
              "      <td>0.116112</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246910</th>\n",
              "      <td>47.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4430</td>\n",
              "      <td>0.161992</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242588</th>\n",
              "      <td>47.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4431</td>\n",
              "      <td>0.157561</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796594</th>\n",
              "      <td>47.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4432</td>\n",
              "      <td>0.161384</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824895</th>\n",
              "      <td>47.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4433</td>\n",
              "      <td>0.117888</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243995</th>\n",
              "      <td>47.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4434</td>\n",
              "      <td>0.187400</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414058</th>\n",
              "      <td>47.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4435</td>\n",
              "      <td>0.228560</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2714148</th>\n",
              "      <td>47.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4436</td>\n",
              "      <td>0.166844</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638386</th>\n",
              "      <td>47.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4437</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3741523</th>\n",
              "      <td>47.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4438</td>\n",
              "      <td>0.128807</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189826</th>\n",
              "      <td>47.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4439</td>\n",
              "      <td>0.106432</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059971</th>\n",
              "      <td>47.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4440</td>\n",
              "      <td>0.175479</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538860</th>\n",
              "      <td>47.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4441</td>\n",
              "      <td>0.170013</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3044423</th>\n",
              "      <td>47.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4442</td>\n",
              "      <td>0.190576</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2918889</th>\n",
              "      <td>47.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4443</td>\n",
              "      <td>0.188502</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3248875</th>\n",
              "      <td>47.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4444</td>\n",
              "      <td>0.148545</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048305</th>\n",
              "      <td>47.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4445</td>\n",
              "      <td>0.209321</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571043</th>\n",
              "      <td>61.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5821</td>\n",
              "      <td>0.015473</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2662014</th>\n",
              "      <td>61.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5822</td>\n",
              "      <td>0.031754</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621199</th>\n",
              "      <td>61.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5823</td>\n",
              "      <td>0.021353</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3889916</th>\n",
              "      <td>61.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5824</td>\n",
              "      <td>0.033428</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2861243</th>\n",
              "      <td>61.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5825</td>\n",
              "      <td>0.016462</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3428729</th>\n",
              "      <td>61.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5826</td>\n",
              "      <td>0.023433</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577897</th>\n",
              "      <td>61.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5827</td>\n",
              "      <td>0.014455</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>61.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89233</th>\n",
              "      <td>61.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5829</td>\n",
              "      <td>0.014425</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>61.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>61.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5831</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>61.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3945991</th>\n",
              "      <td>61.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5833</td>\n",
              "      <td>0.001962</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>61.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5834</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>61.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>61.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>61.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>61.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>61.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5839</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3181119</th>\n",
              "      <td>61.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5840</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>61.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>61.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5842</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>61.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>61.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>61.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>61.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3861950</th>\n",
              "      <td>61.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5847</td>\n",
              "      <td>0.024385</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994036</th>\n",
              "      <td>61.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5848</td>\n",
              "      <td>0.006909</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4008519</th>\n",
              "      <td>61.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5849</td>\n",
              "      <td>0.026236</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3909331</th>\n",
              "      <td>61.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5850</td>\n",
              "      <td>0.016301</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1435 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          day  timestampX  dayOfWeek     x    demand  demandEmptyTimeCount\n",
              "1052018  47.0         0.0        5.0  4416  0.094914                   0.0\n",
              "1947725  47.0         1.0        5.0  4417  0.070080                   1.0\n",
              "419870   47.0         2.0        5.0  4418  0.097505                   0.0\n",
              "3860768  47.0         3.0        5.0  4419  0.105218                   0.0\n",
              "2480279  47.0         4.0        5.0  4420  0.135887                   0.0\n",
              "3590441  47.0         5.0        5.0  4421  0.088575                   0.0\n",
              "966622   47.0         6.0        5.0  4422  0.133793                   0.0\n",
              "2269011  47.0         7.0        5.0  4423  0.132499                   0.0\n",
              "2417416  47.0         8.0        5.0  4424  0.156030                   0.0\n",
              "1403063  47.0         9.0        5.0  4425  0.139918                   0.0\n",
              "3411649  47.0        10.0        5.0  4426  0.120847                   0.0\n",
              "1787719  47.0        11.0        5.0  4427  0.110508                   0.0\n",
              "1413402  47.0        12.0        5.0  4428  0.150303                   0.0\n",
              "1888738  47.0        13.0        5.0  4429  0.116112                   0.0\n",
              "246910   47.0        14.0        5.0  4430  0.161992                   0.0\n",
              "1242588  47.0        15.0        5.0  4431  0.157561                   0.0\n",
              "1796594  47.0        16.0        5.0  4432  0.161384                   0.0\n",
              "2824895  47.0        17.0        5.0  4433  0.117888                   0.0\n",
              "243995   47.0        18.0        5.0  4434  0.187400                   0.0\n",
              "1414058  47.0        19.0        5.0  4435  0.228560                   0.0\n",
              "2714148  47.0        20.0        5.0  4436  0.166844                   0.0\n",
              "638386   47.0        21.0        5.0  4437  0.132349                   0.0\n",
              "3741523  47.0        22.0        5.0  4438  0.128807                   0.0\n",
              "2189826  47.0        23.0        5.0  4439  0.106432                   0.0\n",
              "3059971  47.0        24.0        5.0  4440  0.175479                   0.0\n",
              "1538860  47.0        25.0        5.0  4441  0.170013                   0.0\n",
              "3044423  47.0        26.0        5.0  4442  0.190576                   0.0\n",
              "2918889  47.0        27.0        5.0  4443  0.188502                   0.0\n",
              "3248875  47.0        28.0        5.0  4444  0.148545                   0.0\n",
              "1048305  47.0        29.0        5.0  4445  0.209321                   0.0\n",
              "...       ...         ...        ...   ...       ...                   ...\n",
              "571043   61.0        61.0        5.0  5821  0.015473                   0.0\n",
              "2662014  61.0        62.0        5.0  5822  0.031754                   1.0\n",
              "1621199  61.0        63.0        5.0  5823  0.021353                   2.0\n",
              "3889916  61.0        64.0        5.0  5824  0.033428                   4.0\n",
              "2861243  61.0        65.0        5.0  5825  0.016462                   5.0\n",
              "3428729  61.0        66.0        5.0  5826  0.023433                   6.0\n",
              "577897   61.0        67.0        5.0  5827  0.014455                   9.0\n",
              "260      61.0        68.0        5.0  5828  0.000000                  12.0\n",
              "89233    61.0        69.0        5.0  5829  0.014425                   7.0\n",
              "261      61.0        70.0        5.0  5830  0.000000                  10.0\n",
              "262      61.0        71.0        5.0  5831  0.000000                  13.0\n",
              "263      61.0        72.0        5.0  5832  0.000000                  10.0\n",
              "3945991  61.0        73.0        5.0  5833  0.001962                  14.0\n",
              "264      61.0        74.0        5.0  5834  0.000000                  15.0\n",
              "265      61.0        75.0        5.0  5835  0.000000                  10.0\n",
              "266      61.0        76.0        5.0  5836  0.000000                  10.0\n",
              "267      61.0        77.0        5.0  5837  0.000000                  15.0\n",
              "268      61.0        78.0        5.0  5838  0.000000                  15.0\n",
              "269      61.0        79.0        5.0  5839  0.000000                  14.0\n",
              "3181119  61.0        80.0        5.0  5840  0.006409                  14.0\n",
              "270      61.0        81.0        5.0  5841  0.000000                  15.0\n",
              "271      61.0        82.0        5.0  5842  0.000000                  13.0\n",
              "272      61.0        83.0        5.0  5843  0.000000                  13.0\n",
              "273      61.0        84.0        5.0  5844  0.000000                  14.0\n",
              "274      61.0        85.0        5.0  5845  0.000000                  14.0\n",
              "275      61.0        86.0        5.0  5846  0.000000                  10.0\n",
              "3861950  61.0        87.0        5.0  5847  0.024385                   5.0\n",
              "994036   61.0        88.0        5.0  5848  0.006909                   3.0\n",
              "4008519  61.0        89.0        5.0  5849  0.026236                   2.0\n",
              "3909331  61.0        90.0        5.0  5850  0.016301                   0.0\n",
              "\n",
              "[1435 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-CMXPsobJm",
        "colab_type": "text"
      },
      "source": [
        "# Model Performance\n",
        "### \"Your model can use features of up to 14 consecutive days from the test dataset, ending at timestamp T and predict T+1 to T+5.\"\n",
        "### \"Geohash coverage: You may assume that the set of geohashes are the same in training dataset and test dataset. The original geohashes are anonymised, but you may assume that adjacency is maintained between the geohashes.\"\"\n",
        "### \"Submissions will be evaluated by RMSE (root mean squared error) averaged over all geohash6, 15-minute-bucket pairs.\""
      ]
    }
  ]
}