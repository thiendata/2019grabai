{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0XNGOH8Rtdf",
        "colab_type": "code",
        "outputId": "5f7352f1-c955-4bff-f064-afe48e2d7878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "!pip install scipy==1.2.0\n",
        "!pip install pmdarima\n",
        "!pip install tbats\n",
        "!pip install python-geohash\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = \"drive/My Drive/training.csv\"\n",
        "dataset = pd.read_csv(path)\n",
        "# read from file option\n",
        "#dataset = pd.read_csv('/content/training.csv')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.16.4)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.21.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.12.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.16.4)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.29.10)\n",
            "Requirement already satisfied: scipy<1.3,>=1.2 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->pmdarima) (0.5.1)\n",
            "Requirement already satisfied: tbats in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tbats) (1.16.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from tbats) (0.0)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.6/dist-packages (from tbats) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tbats) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->tbats) (0.21.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.24.2)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.29.10)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima->tbats) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima->tbats) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima->tbats) (2018.9)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->pmdarima->tbats) (0.5.1)\n",
            "Requirement already satisfied: python-geohash in /usr/local/lib/python3.6/dist-packages (0.8.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujsPMXbRxmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# ignore all future warnings \n",
        "#simplefilter(action='ignore', category=FutureWarning) #hide https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore') #https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
        "\n",
        "from datetime import timedelta  \n",
        "from datetime import datetime\n",
        "import time\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from math import sqrt\n",
        "# mapping\n",
        "import folium\n",
        "from folium import plugins\n",
        "import geohash\n",
        "\n",
        "# models\n",
        "from statsmodels.tsa.arima_model import ARIMA # tried a bit but not sure of parameters to use\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from pmdarima import auto_arima\n",
        "from tbats import TBATS\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from fbprophet import Prophet\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# functions tried to use\n",
        "def getNaive(groups, key_tuple, default):\n",
        "  try:\n",
        "    return groups.get_group(key_tuple).mean().demand\n",
        "  except:\n",
        "    print('returning default as unable to find ', key_tuple)\n",
        "    return default\n",
        "\n",
        "def adjMean(testData, currPred):\n",
        "  finalPred=currPred\n",
        "  meanAdj = [-0.000401695755597994,\t-0.001327695755598,\t0.004418304244402,\t0.00649630424440201,\t0.011890304244402,\t-0.005788695755598,\t-0.018775695755598]\n",
        "  for row in testData.itertuples():\n",
        "    finalPred[i]=currPred[i]+meanAdj[getattr(row,'dayOfWeek')]\n",
        "  return finalPred\n",
        "\n",
        "# typo \n",
        "def rsme(y_actual, y_predicted,model):\n",
        "  result = sqrt(mean_squared_error(y_actual, y_predicted))\n",
        "  #print(round(result,5), ' rsme for model ', model) # left-align for easier seeing\n",
        "  return result\n",
        "\n",
        "def avgRMSE(y_predicted, testDatas, model):\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash],testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing\n",
        "  #return mean(rsmes)\n",
        "\n",
        "# just to see if best estimator is skew\n",
        "def adjRMSE(y_predicted, testDatas, model):\n",
        "  # adj demand up\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash]+0.005,testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing\n",
        "  \n",
        "  # adj demand down\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash]-0.005,testDatas[geohash].demand, model))\n",
        "    max=np.max(rsmes)\n",
        "    min=np.min(rsmes)\n",
        "    \n",
        "  print(round(np.mean(rsmes),5), ' rsme for model ', model, '. Max RMSE is at index', np.where(rsmes==max)[0][0] ,'with ', round(max,5), ',Min RMSE is at index', np.where(rsmes==min)[0][0] ,'with ', round(min,5)) # left-align for easier seeing  \n",
        "  #return mean(rsmes)\n",
        "\n",
        "def RSME(y_predicted, testDatas, model):\n",
        "  rsmes = []\n",
        "  for geohash in testDatas:\n",
        "    rsmes.append(rsme(y_predicted[geohash],testDatas[geohash].demand, model))\n",
        "  print(round(mean(rsmes),5), ' rsme for model ', model) # left-align for easier seeing\n",
        "  #return mean(rsmes)\n",
        "  \n",
        "# handle less than expected data (test Data <5 or trainData empty)\n",
        "def predictUnexpectedData(randGeoHash, trainData, testData):\n",
        "  if(len(trainData) == 0):\n",
        "    print('no train data, just predict based on average of 0.105090695755598')\n",
        "    return np.repeat(0.105090695755598,len(testData))\n",
        "  else:   \n",
        "    print('not enough train data, just predict based last train data')\n",
        "    return np.repeat(trainData[-1:].demand.values,len(testData))\n",
        "  \n",
        "# Update only if invalid\n",
        "def predictInvalidPreds(predictions, geohash):\n",
        "  for i in range(len(predictions)):\n",
        "    pred = predictions[i]\n",
        "    if np.isfinite(pred) == False:\n",
        "      print('updating invalid prediction of ', pred, ' for geohash ', geohash, ' to avg of 0.105090695755598')\n",
        "      predictions[i] = 0.105090695755598\n",
        "    elif pred < 0:\n",
        "      print('updating negative prediction of ', pred, ' for geohash ', geohash, ' to 0')\n",
        "      predictions[i] = 0\n",
        "    elif pred > 1:\n",
        "      print('updating above 1 prediction of ', pred, ' for geohash ', geohash, ' to 1')\n",
        "      predictions[i] = 1\n",
        "  return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ioBce94ckVQ",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3SSpmQAPMbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# created 'x' for easier sorting. datetimeindex don't seem easy to use\n",
        "timestamp_dict = {'0:0':0,\t'0:15':1,\t'0:30':2,\t'0:45':3,\t'1:0':4,\t'1:15':5,\t'1:30':6,\t'1:45':7,\t'2:0':8,\t'2:15':9,\t'2:30':10,\t'2:45':11,\t'3:0':12,\t'3:15':13,\t'3:30':14,\t'3:45':15,\t'4:0':16,\t'4:15':17,\t'4:30':18,\t'4:45':19,\t'5:0':20,\t'5:15':21,\t'5:30':22,\t'5:45':23,\t'6:0':24,\t'6:15':25,\t'6:30':26,\t'6:45':27,\t'7:0':28,\t'7:15':29,\t'7:30':30,\t'7:45':31,\t'8:0':32,\t'8:15':33,\t'8:30':34,\t'8:45':35,\t'9:0':36,\t'9:15':37,\t'9:30':38,\t'9:45':39,\t'10:0':40,\t'10:15':41,\t'10:30':42,\t'10:45':43,\t'11:0':44,\t'11:15':45,\t'11:30':46,\t'11:45':47,\t'12:0':48,\t'12:15':49,\t'12:30':50,\t'12:45':51,\t'13:0':52,\t'13:15':53,\t'13:30':54,\t'13:45':55,\t'14:0':56,\t'14:15':57,\t'14:30':58,\t'14:45':59,\t'15:0':60,\t'15:15':61,\t'15:30':62,\t'15:45':63,\t'16:0':64,\t'16:15':65,\t'16:30':66,\t'16:45':67,\t'17:0':68,\t'17:15':69,\t'17:30':70,\t'17:45':71,\t'18:0':72,\t'18:15':73,\t'18:30':74,\t'18:45':75,\t'19:0':76,\t'19:15':77,\t'19:30':78,\t'19:45':79,\t'20:0':80,\t'20:15':81,\t'20:30':82,\t'20:45':83,\t'21:0':84,\t'21:15':85,\t'21:30':86,\t'21:45':87,\t'22:0':88,\t'22:15':89,\t'22:30':90,\t'22:45':91,\t'23:0':92,\t'23:15':93,\t'23:30':94,\t'23:45':95,}\n",
        "try:\n",
        "  dataset['timestampX']=dataset['timestamp']\n",
        "  dataset.timestampX=dataset.timestampX.map(timestamp_dict) #replace is so slow\n",
        "  dataset['dayOfWeek']=dataset['day']%7\n",
        "except:\n",
        "  print('unexpected error during copy')\n",
        " \n",
        "dataset['x']=(dataset.day-1)*96 + dataset.timestampX\n",
        "#origData = dataset.copy()\n",
        "dataset['datetime']=pd.to_timedelta(dataset.loc[:,'day'].astype(str) + ' days ' + dataset.loc[:,'timestamp'] + \":00\")\n",
        "dataset.datetime=pd.to_datetime(dataset.loc[:,'datetime'])\n",
        "\n",
        "dataset=dataset.sort_values(by=['geohash6', 'x'])\n",
        "#dataset.head()\n",
        "\n",
        "# just store last sequential demand\n",
        "# TODO should not store demand of prev since unknown? \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUMKeaKWVaoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def splitData1(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1) # store last demand \n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1) # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand[1:] #remove first row since NA  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)[1:]\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# only diff from 1 is to drop row when don't have prev interval\n",
        "def splitData2(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1)[1:] # store last demand and remove first row since NA\n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1)[1:] # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainData=trainData[trainData.prevX==trainData.x-1] #don't do on test data else only accept those with sequential (though final test should be)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1) \n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# only diff from 2 is to drop prevX too\n",
        "def splitData3(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1)[1:] # store last demand and remove first row since NA\n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1)[1:] # for checking if got prev interval\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainData=trainData[trainData.prevX==trainData.x-1] #don't do on test data else only accept those with sequential (though final test should be)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','prevX','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','prevX','demand'], axis=1) \n",
        "  return trainData,trainLabel,testData\n",
        "# use neighbor \n",
        "def splitData4(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  hashDataset1['prevDemand']=hashDataset.demand.shift(1) # store last demand \n",
        "  hashDataset1['prevX']=hashDataset.x.shift(1) # for checking if got prev interval\n",
        "  \n",
        "  neighborDemand=dataset[dataset['geohash6'].isin(geohash.neighbors(randGeoHash)) & dataset['x'].isin(hashDataset1.x)].groupby('x').mean()\n",
        "  neighborDemand['neighborPrevDemand']=neighborDemand.demand.shift(1)\n",
        "  hashDataset1=pd.merge(hashDataset1, neighborDemand[['neighborPrevDemand']], on='x')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  # remove first row since NA\n",
        "  trainLabel=trainData.demand[1:]  \n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)[1:]\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data\n",
        "def splitData5(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand','dayOfWeek'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand','dayOfWeek'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data with zero for empty geohash\n",
        "def splitData6(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # for consistency, testData is still the last 5 with demand  \n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  missingRows=pd.DataFrame() \n",
        "  missingRows['x']=missing_elements(trainData.x.values)\n",
        "  missingRows['demand']=0    \n",
        "  missingRows['day']=np.floor(missingRows['x']/96)+1\n",
        "  missingRows['dayOfWeek']=missingRows['day']%7\n",
        "  missingRows['timestampX']=np.floor(missingRows['x']%96)\n",
        "  trainData=trainData6.append(missingRows,sort=False)   \n",
        "  trainData=trainData6.sort_values('x')  \n",
        "    \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# plain data with median field\n",
        "def splitData7(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  newFrame=pd.DataFrame()\n",
        "  newFrame['dayOfWeek']=range(0,6)\n",
        "  newFrame['dayOfWeekMedian']=dataset.groupby(['dayOfWeek']).demand.median()\n",
        "  hashDataset1=pd.merge(hashDataset1,newFrame,on='dayOfWeek')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def splitData8(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  newFrame=pd.DataFrame()\n",
        "  newFrame['dayOfWeek']=range(0,6)\n",
        "  newFrame['dayOfWeekMedian']=dataset.groupby(['dayOfWeek']).demand.median()\n",
        "  hashDataset1=pd.merge(hashDataset1,newFrame,on='dayOfWeek')\n",
        "  \n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand','prevDemand','prevX'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand','prevDemand','prevX'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def splitData9(hashDataset, allSet):  \n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # prev neighbor data within 14 days\n",
        "  neighborDemand=dataset[(dataset.day>=testData.iloc[0].day) & (dataset.x<testData.iloc[0].x) & dataset['geohash6'].isin(geohash.neighbors(randGeoHash))].groupby(['geohash6'])\n",
        "  neighborDemandMean=neighborDemand.demand.mean().values\n",
        "  neighborDemandMax=neighborDemand.demand.max().values\n",
        "  for i in range(8):\n",
        "    try:\n",
        "      hashDataset1['nMean'+str(i)]=neighborDemandMean[i]\n",
        "      hashDataset1['nMax'+str(i)]=neighborDemandMax[i]\n",
        "    except IndexError:\n",
        "      hashDataset1['nMax'+str(i)]=hashDataset1['nMean'+str(i)]=0\n",
        "\n",
        "  testData=hashDataset1[-5:]\n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "# create count of zeros field by timestamp\n",
        "def splitData10(hashDataset, allSet):\n",
        "  hashDataset1=hashDataset.copy()\n",
        "  testData=hashDataset1[-5:]\n",
        "  # for consistency, testData is still the last 5 with demand  \n",
        "  trainData=hashDataset1.loc[(hashDataset1.day>=testData.iloc[0].day-14) & (hashDataset1.x<testData.iloc[0].x)] \n",
        "  if(len(trainData)>1):\n",
        "    missingRows=pd.DataFrame()   \n",
        "    mElements=missing_elements(trainData.x.values)\n",
        "    missingRows['x']=mElements\n",
        "    missingRows['demand']=0    \n",
        "    missingRows['day']=np.floor(missingRows['x']/96)+1\n",
        "    missingRows['dayOfWeek']=missingRows['day']%7\n",
        "    missingRows['timestampX']=np.floor(missingRows['x']%96)\n",
        "    trainData=trainData.append(missingRows,sort=False)\n",
        "    trainData=trainData.sort_values('x')  \n",
        "\n",
        "  emptyTime=trainData[trainData.demand==0].groupby('timestampX').demand.count()\n",
        "  trainData=trainData.join(emptyTime,on='timestampX',rsuffix='EmptyTimeCount')\n",
        "  testData=testData.join(emptyTime,on='timestampX',rsuffix='EmptyTimeCount')\n",
        "  trainData.demandEmptyTimeCount.fillna(0, inplace=True)\n",
        "  testData.demandEmptyTimeCount.fillna(0, inplace=True)\n",
        "  \n",
        "  trainLabel=trainData.demand\n",
        "  testData = testData.drop(['geohash6','timestamp','datetime','demand'], axis=1)\n",
        "  trainData = trainData.drop(['geohash6','timestamp','datetime','demand'], axis=1)  \n",
        "  return trainData,trainLabel,testData\n",
        "\n",
        "def missing_elements(L):\n",
        "    start, end = L[0], L[-1]\n",
        "    return sorted(set(range(start, end + 1)).difference(L))\n",
        "\n",
        "# for rnn\n",
        "def create_RNNset(trainData,trainLabel,testData):\n",
        "  trainX = np.array(trainData.x).reshape(len(trainData),1,-1)\n",
        "  trainY = trainLabel\n",
        "  testX =  np.array(testData.x).reshape(len(testData),1,-1)\n",
        "  \n",
        "  return trainX,trainY,testX\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvAyJD2MBSgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed4c19cc-0bc2-40a7-d2d4-4b701ca1c1ce"
      },
      "source": [
        "# for debugging\n",
        "len(dataset.geohash6.unique())"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjZryQ4V45H-",
        "colab_type": "text"
      },
      "source": [
        "# Core Model Running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6e0LMm8s5k4",
        "colab_type": "code",
        "outputId": "191e12d1-446c-4e58-9b64-6886a09ae88f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        }
      },
      "source": [
        "# TODO randomise the start day and timestamp to do prediction for each geohash prior to running models\n",
        "# assume sorted from small to big already\n",
        "randGeoHashes=dataset.geohash6.unique()\n",
        "#randGeoHashes=np.random.choice(dataset.geohash6.unique(),30)\n",
        "#randGeoHashes=['qp02yu','qp02yv','qp03zx', 'qp08fu', 'qp090p','qp03q4', 'qp092d','qp09jy','qp09dz','qp03qp']\n",
        "\n",
        "# stores preds as dict for easier \n",
        "preds={}\n",
        "tests=['predictedTSDOW',\n",
        "'predictedTS',\n",
        "'predictedDOW',\n",
        "'predictedArima1',\n",
        "'predictedArima2',\n",
        "'predictedArima3',\n",
        "'predictedXGBoost1',\n",
        "'predictedXGBoost2',\n",
        "'predictedXGBoost3',\n",
        "'predictedXGBoost4',\n",
        "'predictedXGBoost5-perfect',\n",
        "'predictedXGBoost6',       \n",
        "'predictedXGBoost7-testData4',\n",
        "'predictedXGBoost8',       \n",
        "'predictedXGBoost9',       \n",
        "'predictedTBATS',\n",
        "'predictedRandForestRegressor1',\n",
        "'predictedRandForestRegressor2',\n",
        "'predictedRandForestRegressor3',\n",
        "'predictedRandForestRegressor4-perfect',\n",
        "'predictedRandForestRegressor5',\n",
        "'predictedRandForestRegressor6',\n",
        "'predictedRandForestRegressor7',\n",
        "'predictedRandForestRegressor8',       \n",
        "'predictedRandForestRegressor9',       \n",
        "'gradientBoostRegressor1',\n",
        "'gradientBoostRegressor2',\n",
        "'gradientBoostRegressor3',\n",
        "'gradientBoostRegressor4',\n",
        "'gradientBoostRegressor5',       \n",
        "'gradientBoostRegressor6',\n",
        "'gradientBoostRegressor7',    \n",
        "'logReg'\n",
        "'gaussian',       \n",
        "'lgm1',\n",
        "'SGDR',\n",
        "'keras1',\n",
        "'keras2',\n",
        "'keras3-notdone',\n",
        "'fb-prophet',\n",
        "'fb-prophet2'\n",
        "]\n",
        "for pred in tests:\n",
        "  preds[pred]={}\n",
        "\n",
        "# this is to know which tests was really used\n",
        "workingHashForPrinting=''\n",
        "testDatas = {}\n",
        "\n",
        "# idea as keras is so slow to train and more epoch usually better. just retrain one over time instead of new 1 per geohash\n",
        "#keras2 = Sequential()\n",
        "#keras2.add(LSTM(4, input_shape=(1, 1)))\n",
        "#keras2.add(Dense(1))\n",
        "#keras2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# idea 2 is to train keras over whole data then just predict\n",
        "#keras3 = Sequential()\n",
        "#keras3.add(LSTM(4, input_shape=(1, 1)))\n",
        "#keras3.add(Dense(1))\n",
        "#keras3.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# reuse and just keep fitting\n",
        "gbrt3=GradientBoostingRegressor(n_estimators=100)\n",
        "reg8 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "xg_reg8 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True)\n",
        "    \n",
        "# variables for model in following sections\n",
        "# naive, auto-arima, xgboost + GBR, tbats, randforestreg, rnn, FB prophet\n",
        "boolSectionsToRun = [False,False,True,False,False,False,False]\n",
        "showGraph=False\n",
        "showTimeTakenForEach=False\n",
        "for randGeoHash in randGeoHashes:\n",
        "  print('geohash is ',randGeoHash)\n",
        "  \n",
        "  #print('neighbours are ',geohash.neighbors(randGeoHash))\n",
        "  randDataSet=dataset.loc[dataset.geohash6==randGeoHash]\n",
        "  # abnormal data\n",
        "  # qp02yu had only 2 points so cannot really predict since assume T+1 to T+5\n",
        "  # qp02yv if choose 5 test data (start from day 24), there is only 1 training data within 14 days\n",
        "  # TODO qp03zx, qp08fu auto arima return nan. not sure why\n",
        "  # qp09jy got negative indices \n",
        "  # Could not successfully fit ARIMA to input data. It is likely your data is non-stationary. Please induce stationarity or try a different range of model order params. If your data is seasonal, check the period (m) of the data.\n",
        "  \n",
        "  # cap at 14 days as per requirement\n",
        "  # since data has shortfall for some, take from backwards  \n",
        "  testData=randDataSet[-5:]\n",
        "  trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]\n",
        "  testDatas[randGeoHash] = randDataSet[-5:]\n",
        "  \n",
        "  # sklearn modules prefer 2d array\n",
        "  if(len(trainData)) > 0:\n",
        "    trainDataX = np.array(trainData.x).reshape(len(trainData),-1)\n",
        "  testDataX = np.array(testData.x).reshape(len(testData),-1)     \n",
        "  \n",
        "  # for xgboost, randomforest  \n",
        "  trainData1,trainLabel1,testData1=splitData1(randDataSet, dataset)      \n",
        "  trainData2,trainLabel2,testData2=splitData2(randDataSet, dataset)\n",
        "  trainData3,trainLabel3,testData3=splitData3(randDataSet, dataset)  \n",
        "  #trainData4,trainLabel4,testData4=splitData4(randDataSet, dataset)\n",
        "  trainData5,trainLabel5,testData5=splitData5(randDataSet, dataset)\n",
        "  # don't use as didn't catch exception for missing_elements in rare case\n",
        "  #trainData6,trainLabel6,testData6=splitData6(randDataSet, dataset)\n",
        "  #trainData7,trainLabel7,testData7=splitData7(randDataSet, dataset)\n",
        "  trainData9,trainLabel9,testData9=splitData9(randDataSet, dataset)\n",
        "  trainData10,trainLabel10,testData10=splitData10(randDataSet, dataset)\n",
        "  trainDataForTest = trainData.drop(['geohash6','timestamp','datetime', 'day', 'dayOfWeek', 'x', 'timestampX'], axis=1) \n",
        "  testDataForTest = testData.drop(['geohash6','timestamp','datetime','day', 'dayOfWeek', 'x', 'timestampX'], axis=1) \n",
        "  \n",
        "  # for robustness, predict unexpected data set (too few training or testing)\n",
        "  if(len(randDataSet) <= 5 or len(trainData) <= 1 or len(trainData1) <= 1  or len(trainData2) <= 1 ):\n",
        "    result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    for pred in tests:\n",
        "      preds[pred][randGeoHash]=result\n",
        "    continue\n",
        "  \n",
        "  if(showGraph):\n",
        "    plt.title('train data for geohash ' + randGeoHash)    \n",
        "    ax1=trainData.plot(x='x', y='demand')    \n",
        "    testData.plot(ax=ax1, x='x', y='demand') #, ax=ax1 if want to see in same\n",
        "\n",
        "  workingHashForPrinting=randGeoHash\n",
        "  for pred in tests:\n",
        "    preds[pred][randGeoHash] = []  \n",
        "  start = time.clock()   \n",
        "  \n",
        "  # Section 0 - naive self-coded with default demand\n",
        "  # super naive prediction based on historical day and timestamp \n",
        "  # 14-days of data too sparse that not much diff when tried median\n",
        "  if(boolSectionsToRun[0]):\n",
        "    meanTrainDataBy_TSDOW=trainData.groupby(['timestamp','dayOfWeek'])\n",
        "    meanTrainDataBy_TS=trainData.groupby(['timestamp'])\n",
        "    meanTrainDataBy_DOW=trainData.groupby(['dayOfWeek'])\n",
        "    defaultDemand=0.10509069575559817 # based on mean\n",
        "    for i in range(len(testData)):\n",
        "      predictedTSDOW[randGeoHash] =(getNaive(meanTrainDataBy_TSDOW, (testData.iloc[i].timestamp, testData.iloc[i].dayOfWeek), defaultDemand))\n",
        "      predictedTS[randGeoHash] = (getNaive(meanTrainDataBy_TS, (testData.iloc[i].timestamp), defaultDemand))\n",
        "      predictedDOW[randGeoHash] = (getNaive(meanTrainDataBy_DOW, (testData.iloc[i].dayOfWeek), defaultDemand))\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for naive \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 1 - auto arima\n",
        "  #https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n",
        "  # TODO auto-arima use step rather than respective X\n",
        "  if(boolSectionsToRun[1]):    \n",
        "    try:\n",
        "      arima_model = auto_arima(trainData.demand, error_action='ignore', suppress_warnings=True)        \n",
        "      model_fit = arima_model.fit(trainData.demand, suppress_warnings=True)\n",
        "      preds['predictedArima1'][randGeoHash] = predictInvalidPreds(model_fit.predict(n_periods=len(testData)), randGeoHash)\n",
        "      # seem like bug in auto_arima library for geohash6 qp09jy. too many indices when run something like this arima_model1=auto_arima([0.03,0.35])\n",
        "    except IndexError:\n",
        "      print('unexpected indexerror in arima')\n",
        "      preds['predictedArima1'][randGeoHash]=predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    # assumption that the last 5 data point is of most importance\n",
        "    try:\n",
        "      arima_model2 = auto_arima(trainData[-5:].demand, suppress_warnings=True)\n",
        "      model_fit2 = arima_model2.fit(trainData[-5:].demand, suppress_warnings=True)\n",
        "      preds['predictedArima2'][randGeoHash] = predictInvalidPreds(model_fit2.predict(n_periods=len(testData)), randGeoHash)\n",
        "    except IndexError:\n",
        "      print('unexpected indexerror in arima')\n",
        "      preds['predictedArima2'][randGeoHash]=predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    # assumption 7 days * 24hrs * 4/hour records per season\n",
        "    #arima_model3 = auto_arima(trainData.demand, error_action=m=7*24*4, suppress_warnings=True)\n",
        "    #model_fit3 = arima_model3.fit(trainData.demand, suppress_warnings=True)\n",
        "    #predictedArima3[randGeoHash] = predictInvalidPreds(model_fit3.predict(n_periods=len(testData)), randGeoHash)\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for auto arima \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 2 - xgboost and others\n",
        "  #https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "  #https://www.kaggle.com/mburakergenc/predictions-with-xgboost-and-linear-regression\n",
        "  #https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6 on tuning gamma\n",
        "  if(boolSectionsToRun[2]):\n",
        "    xg_reg1 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000, silent=True)\n",
        "    xg_reg1.fit(trainData10, trainLabel10)\n",
        "    preds['predictedXGBoost1'][randGeoHash] = predictInvalidPreds(xg_reg1.predict(testData10), randGeoHash)\n",
        "\n",
        "    # not sure why but this is significant different than others\n",
        "    xg_reg2 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True)\n",
        "    xg_reg2.fit(trainData1, trainLabel1)\n",
        "    preds['predictedXGBoost2'][randGeoHash] = predictInvalidPreds(xg_reg2.predict(testData1), randGeoHash)\n",
        "    \n",
        "    # from gridsearch except n_estimators = 1000, etc\n",
        "    #xg_reg3 = xgb.XGBRegressor(n_estimators = 1000, alpha=0, colsample_bytree=0.3, gamma=0.5, learning_rate=0.1, max_depth=7, min_child_weight=10, subsample=1.0,silent=True)\n",
        "    #xg_reg3.fit(trainData1, trainLabel1)\n",
        "    #preds['predictedXGBoost3'][randGeoHash] = predictInvalidPreds(xg_reg3.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #xg_reg4 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #    min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg4.fit(trainData2, trainLabel2)\n",
        "    #preds['predictedXGBoost4'][randGeoHash] = predictInvalidPreds(xg_reg4.predict(testData2), randGeoHash)\n",
        "    \n",
        "    # just to confirm model ok (use demand to predict demand)\n",
        "    #xg_reg5 = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7, silent=True)\n",
        "    #xg_reg5.fit(trainDataForTest, trainData.demand)\n",
        "    #preds['predictedXGBoost5-perfect'][randGeoHash] = predictInvalidPreds(xg_reg5.predict(testDataForTest), randGeoHash)\n",
        "\n",
        "    #xg_reg6 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #     min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg6.fit(trainData3, trainLabel3)\n",
        "    #preds['predictedXGBoost6'][randGeoHash] = predictInvalidPreds(xg_reg6.predict(testData3), randGeoHash)\n",
        "    \n",
        "    #xg_reg7 = xgb.XGBRegressor(eval_metric='rmse', base_score=0.1, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
        "    #     min_child_weight=1, missing=None, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
        "    #     scale_pos_weight=1, seed=0, silent=True, subsample=0.75)\n",
        "    #xg_reg7.fit(trainData4, trainLabel4)\n",
        "    #preds['predictedXGBoost7-testData4'][randGeoHash] = predictInvalidPreds(xg_reg7.predict(testData4), randGeoHash)\n",
        "    \n",
        "    #xg_reg8.fit(trainData1, trainLabel1)\n",
        "    #preds['predictedXGBoost8'][randGeoHash] = predictInvalidPreds(xg_reg8.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #xg_reg9 = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.01, max_depth = 7, alpha = 10, n_estimators = 1000,silent=True).fit(trainData9, trainLabel9)\n",
        "    #preds['predictedXGBoost9'][randGeoHash] = predictInvalidPreds(xg_reg9.predict(testData9), randGeoHash)\n",
        "    \n",
        "    \n",
        "    #https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.d65q9kG1.dpuf\n",
        "    gbrt=GradientBoostingRegressor(n_estimators=100).fit(trainData1, trainLabel1)\n",
        "    preds['gradientBoostRegressor1'][randGeoHash] = predictInvalidPreds(gbrt.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    \n",
        "    #gbrt2=GradientBoostingRegressor(n_estimators=100).fit(trainData7, trainLabel7)\n",
        "    #preds['gradientBoostRegressor2'][randGeoHash] = predictInvalidPreds(gbrt2.predict(testData7), randGeoHash)    \n",
        "     \n",
        "      \n",
        "    #gbrt3.fit(trainData1, trainLabel1)\n",
        "    #preds['gradientBoostRegressor3'][randGeoHash] = predictInvalidPreds(gbrt3.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    # from gridsearch based on gbrt1\n",
        "    gbrt4=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)\n",
        "    gbrt4.fit(trainData1, trainLabel1)\n",
        "    preds['gradientBoostRegressor4'][randGeoHash] = predictInvalidPreds(gbrt4.predict(testData1), randGeoHash)    \n",
        "        \n",
        "    #gbrt5=GradientBoostingRegressor(n_estimators=1000, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)\n",
        "    #gbrt5.fit(trainData1, trainLabel1)\n",
        "    #preds['gradientBoostRegressor5'][randGeoHash] = predictInvalidPreds(gbrt5.predict(testData1), randGeoHash)    \n",
        "    \n",
        "    gbrt6=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,subsample=0.8).fit(trainData9, trainLabel9)\n",
        "    preds['gradientBoostRegressor6'][randGeoHash] = predictInvalidPreds(gbrt6.predict(testData9), randGeoHash)  \n",
        "    \n",
        "    # from gridsearch based on gbrt1\n",
        "    gbrt7=GradientBoostingRegressor(n_estimators=40, max_depth=1,learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,subsample=0.8)\n",
        "    gbrt7.fit(trainData10, trainLabel10)\n",
        "    preds['gradientBoostRegressor7'][randGeoHash] = predictInvalidPreds(gbrt7.predict(testData10), randGeoHash)    \n",
        "    \n",
        "    #from sklearn.linear_model import LogisticRegression\n",
        "    # need to change shape and not done\n",
        "    #logReg = LogisticRegression().fit(trainData1, testData1)\n",
        "    #preds['logReg'][randGeoHash] = predictInvalidPreds(logReg.predict(testData1), randGeoHash)\n",
        "    \n",
        "    #from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "    #gpr = GaussianProcessRegressor(normalize_y=True).fit(trainData1, trainLabel1)\n",
        "    #preds['gaussian'][randGeoHash] = predictInvalidPreds(gpr.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    #from sklearn.linear_model import SGDRegressor\n",
        "    #sgdr=SGDRegressor(max_iter=1000, tol=1e-3).fit(trainData1, trainLabel1)\n",
        "    #preds['SGDR'][randGeoHash] = predictInvalidPreds(gpr.predict(testData1) , randGeoHash)    \n",
        "    \n",
        "    #https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
        "    #import lightgbm as lgb\n",
        "    #train_data=lgb.Dataset(trainData1,label=trainLabel1)\n",
        "    #params = {'learning_rate':0.001}\n",
        "    #model= lgb.train(params, train_data, 100)\n",
        "    #preds['lgm1'][randGeoHash] = predictInvalidPreds(model.predict(testData1), randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for xgboost \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 3 - TBATS\n",
        "  #https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a\n",
        "  if(boolSectionsToRun[3]):\n",
        "    estimator = TBATS()  \n",
        "    fitted_model = estimator.fit(trainData.demand)  \n",
        "    preds['predictedTBATS'][randGeoHash] = predictInvalidPreds(fitted_model.forecast(steps=len(testData)), randGeoHash)\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for TBATS \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "  # Section 4 - Random Forest\n",
        "  #https://github.com/sdaulton/TaxiPrediction/blob/master/5b.%20Destinations%20-%20Random%20Forest.ipynb\n",
        "  if(boolSectionsToRun[4]):\n",
        "    #reg1 = RandomForestRegressor(n_estimators=1, max_depth=20, n_jobs=-1,warm_start=True) \n",
        "    #reg1.fit(trainData1,trainLabel1) \n",
        "    #preds['predictedRandForestRegressor1'][randGeoHash] = predictInvalidPreds(reg1.predict(testData1), randGeoHash)\n",
        "\n",
        "    #reg2 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True) \n",
        "    #reg2.fit(trainData1,trainLabel1) \n",
        "    # preds['predictedRandForestRegressor2'][randGeoHash] = predictInvalidPreds(reg2.predict(testData1), randGeoHash)\n",
        "\n",
        "    reg3 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    reg3.fit(trainData1,trainLabel1)\n",
        "    preds['predictedRandForestRegressor3'][randGeoHash] = predictInvalidPreds(reg3.predict(testData1), randGeoHash)\n",
        "    \n",
        "    reg4 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)\n",
        "    reg4.fit(trainDataForTest,trainData.demand)\n",
        "    preds['predictedRandForestRegressor4-perfect'][randGeoHash] = predictInvalidPreds(reg4.predict(testDataForTest), randGeoHash)\n",
        "    \n",
        "    #reg5 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg5.fit(trainData2,trainLabel2)\n",
        "    #preds['predictedRandForestRegressor5'][randGeoHash] = predictInvalidPreds(reg5.predict(testData2), randGeoHash)\n",
        "    \n",
        "    #reg6 = RandomForestRegressor(n_estimators=20, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg6.fit(trainData3,trainLabel3)\n",
        "    #preds['predictedRandForestRegressor6'][randGeoHash] = predictInvalidPreds(reg6.predict(testData3), randGeoHash)\n",
        "    \n",
        "    #reg7 = RandomForestRegressor(n_estimators=100, max_depth=20, n_jobs=-1, warm_start=True)        \n",
        "    #reg7.fit(trainData4,trainLabel4)\n",
        "    #preds['predictedRandForestRegressor7'][randGeoHash] = predictInvalidPreds(reg7.predict(testData4), randGeoHash)\n",
        "        \n",
        "    reg8.fit(trainData1,trainLabel1)\n",
        "    preds['predictedRandForestRegressor8'][randGeoHash] = predictInvalidPreds(reg8.predict(testData1), randGeoHash)\n",
        "    \n",
        "\n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for randforest \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "   # Section 5 - keras RNN \n",
        "   #https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
        "  if(boolSectionsToRun[5]):\n",
        "    train5X,train5Y,test5X = create_RNNset(trainData5, trainLabel5, testData5)     \n",
        "    \n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #train4X = scaler.fit_transform(dataset)\n",
        "    #test4X = scaler.fit_transform(dataset)        \n",
        "    #test4Y = scaler.inverse_transform([testY])\n",
        "    \n",
        "    #keras1 = Sequential()\n",
        "    #keras1.add(LSTM(4, input_shape=(1, 1)))\n",
        "    #keras1.add(Dense(1))\n",
        "    #keras1.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    #keras1.fit(train5X, train5Y, epochs=5, batch_size=1, verbose=0)    \n",
        "    #preds['keras1'][randGeoHash] = predictInvalidPreds(keras1.predict(test5X), randGeoHash)\n",
        "    \n",
        "    keras2.fit(train5X, train5Y, epochs=5, batch_size=1, verbose=0)    \n",
        "    preds['keras2'][randGeoHash] = predictInvalidPreds(keras2.predict(test5X), randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for keras \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "   \n",
        "  if(boolSectionsToRun[6]):\n",
        "    m = Prophet()\n",
        "    trainData['ds']=trainData['datetime']\n",
        "    trainData['cap']=1\n",
        "    trainData['y']=trainData['demand']\n",
        "    testData['ds']=testData['datetime']\n",
        "    testData['cap']=1\n",
        "    \n",
        "    m.fit(trainData)\n",
        "    preds['fb-prophet'][randGeoHash] = m.predict(testData)\n",
        "    preds['fb-prophet'][randGeoHash] = predictInvalidPreds(preds['fb-prophet'][randGeoHash]['yhat'].values, randGeoHash)\n",
        "    \n",
        "    m2 = Prophet(growth = 'logistic')\n",
        "    m2.fit(trainData)\n",
        "    preds['fb-prophet2'][randGeoHash] = m2.predict(testData)\n",
        "    preds['fb-prophet2'][randGeoHash] = predictInvalidPreds(preds['fb-prophet2'][randGeoHash]['yhat'].values, randGeoHash)\n",
        "    \n",
        "    \n",
        "    if(showTimeTakenForEach):\n",
        "      print(\"time taken for fb-prophet \", time.clock() - start)\n",
        "      start = time.clock()\n",
        "# Result comparison at end\n",
        "# just loop all and print if a working hash was not empty\n",
        "for pred in tests:\n",
        "    if(len(preds[pred][workingHashForPrinting]) > 0):\n",
        "      avgRMSE(preds[pred], testDatas, pred)\n",
        "\n",
        "# run one more time using fully trained models, result was worse off\n",
        "trainedpreds={}\n",
        "trainedTests=['gbrt1grid']#'gbrt3','keras2'] #'reg8','xg_reg8'\n",
        "\n",
        "# see if regressor can be improved \n",
        "#adjRMSE(preds['gradientBoostRegressor4'],testDatas,'gbrt4 adj')\n",
        "#adjRMSE(preds['gradientBoostRegressor5'],testDatas,'gbrt5 adj')\n",
        "\n",
        "# not running as no improvement from a general model\n",
        "if(False):\n",
        "  for pred in trainedTests:\n",
        "    trainedpreds[pred]={}\n",
        "\n",
        "  #See more at: https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.Bwu6NdCJ.dpuf\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n",
        "    param_grid = {'n_estimators':range(20,81,10), 'max_depth':range(1,20)}, n_jobs=4,iid=False, cv=5)\n",
        "  \n",
        "  \n",
        "  for randGeoHash in randGeoHashes:  \n",
        "    randDataSet=dataset[dataset.geohash6==randGeoHash]\n",
        "    testData=randDataSet[-5:]\n",
        "    trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]  \n",
        "    if(len(randDataSet) <= 5 or len(trainData) <= 1):\n",
        "      result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "      for pred in trainedpreds:\n",
        "        trainedpreds[pred][randGeoHash]=result\n",
        "      continue  \n",
        "          \n",
        "    \n",
        "    trainData1,trainLabel1,testData1=splitData1(dataset[dataset.geohash6==randGeoHash], dataset)      \n",
        "    gsearch1.fit(trainData1,trainLabel1)\n",
        "    #trainedpreds['gbrt3'][randGeoHash]=gbrt3.predict(testData1)\n",
        "\n",
        "    #trainData5,trainLabel5,testData5=splitData1(randDataSet, dataset)      \n",
        "    #train5X,train5Y,test5X = create_RNNset(trainData5, trainLabel5, testData5) \n",
        "    #trainedpreds['keras2'][randGeoHash] = predictInvalidPreds(keras2.predict(test5X), randGeoHash)\n",
        "\n",
        "  #for pred in trainedTests:    \n",
        "  #  avgRMSE(trainedpreds[pred], testDatas, pred)\n",
        "\n",
        "  \n",
        "# Comments\n",
        "# AutoArima / TBATS slow and couldn't put in missing data logically. tried RNN, FB Prope in others pyb and was slow\n",
        "# using tree for time series is weird too\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "geohash is  qp02yc\n",
            "updating negative prediction of  -0.000908746189200001  for geohash  qp02yc  to 0\n",
            "geohash is  qp02yf\n",
            "updating negative prediction of  -0.00037521124  for geohash  qp02yf  to 0\n",
            "updating negative prediction of  -7.134676e-05  for geohash  qp02yf  to 0\n",
            "updating negative prediction of  -0.00022425218782836943  for geohash  qp02yf  to 0\n",
            "geohash is  qp02yu\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "geohash is  qp02yv\n",
            "not enough train data, just predict based last train data\n",
            "geohash is  qp02yy\n",
            "updating negative prediction of  -0.00014895201  for geohash  qp02yy  to 0\n",
            "updating negative prediction of  -0.00011217594  for geohash  qp02yy  to 0\n",
            "updating negative prediction of  -8.106232e-05  for geohash  qp02yy  to 0\n",
            "geohash is  qp02yz\n",
            "geohash is  qp02z1\n",
            "updating negative prediction of  -0.0008443898783570172  for geohash  qp02z1  to 0\n",
            "updating negative prediction of  -0.0008443898783570172  for geohash  qp02z1  to 0\n",
            "geohash is  qp02z3\n",
            "geohash is  qp02z4\n",
            "geohash is  qp02z5\n",
            "geohash is  qp02z6\n",
            "updating negative prediction of  -0.006103754  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.005628228  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.0033970475  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.0017606260796718116  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.0017606260796718116  for geohash  qp02z6  to 0\n",
            "updating negative prediction of  -0.0017606260796718116  for geohash  qp02z6  to 0\n",
            "geohash is  qp02z7\n",
            "geohash is  qp02z9\n",
            "geohash is  qp02zc\n",
            "updating negative prediction of  -0.008777022  for geohash  qp02zc  to 0\n",
            "geohash is  qp02zd\n",
            "updating negative prediction of  -0.011222541  for geohash  qp02zd  to 0\n",
            "updating negative prediction of  -0.0079315305  for geohash  qp02zd  to 0\n",
            "updating negative prediction of  -3.9505431943203e-05  for geohash  qp02zd  to 0\n",
            "geohash is  qp02ze\n",
            "updating negative prediction of  -0.014687598  for geohash  qp02ze  to 0\n",
            "updating negative prediction of  -0.01428026  for geohash  qp02ze  to 0\n",
            "updating negative prediction of  -0.0030896290307346483  for geohash  qp02ze  to 0\n",
            "updating negative prediction of  -0.0030896290307346483  for geohash  qp02ze  to 0\n",
            "geohash is  qp02zf\n",
            "geohash is  qp02zg\n",
            "updating negative prediction of  -0.0026205911510161365  for geohash  qp02zg  to 0\n",
            "updating negative prediction of  -0.0026205911510161365  for geohash  qp02zg  to 0\n",
            "geohash is  qp02zh\n",
            "updating negative prediction of  -0.011052489  for geohash  qp02zh  to 0\n",
            "updating negative prediction of  -0.01232636  for geohash  qp02zh  to 0\n",
            "updating negative prediction of  -0.017567933  for geohash  qp02zh  to 0\n",
            "geohash is  qp02zj\n",
            "updating negative prediction of  -0.015937507  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.027340949  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.034136295  for geohash  qp02zj  to 0\n",
            "updating negative prediction of  -0.0046875477  for geohash  qp02zj  to 0\n",
            "geohash is  qp02zk\n",
            "updating negative prediction of  -0.0008595586  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.016199768  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.016199768  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.012077749  for geohash  qp02zk  to 0\n",
            "updating negative prediction of  -0.017977536  for geohash  qp02zk  to 0\n",
            "geohash is  qp02zm\n",
            "geohash is  qp02zn\n",
            "geohash is  qp02zp\n",
            "geohash is  qp02zq\n",
            "geohash is  qp02zr\n",
            "updating negative prediction of  -0.008030236  for geohash  qp02zr  to 0\n",
            "updating negative prediction of  -0.015254557  for geohash  qp02zr  to 0\n",
            "geohash is  qp02zs\n",
            "geohash is  qp02zt\n",
            "updating negative prediction of  -0.0037279725  for geohash  qp02zt  to 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZKFdukWLuT",
        "colab_type": "text"
      },
      "source": [
        "# GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBQerj_T3dI",
        "colab_type": "code",
        "outputId": "4a841dab-b756-4421-c649-34de085dd58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "for pred in trainedTests:\n",
        "  trainedpreds[pred]={}\n",
        "\n",
        "XGparams = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.3,0.6, 0.8, 1.0],\n",
        "        'max_depth': [1, 3, 4, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'alpha': [0, 10]\n",
        "        }\n",
        "  \n",
        "#See more at: https://shankarmsy.github.io/stories/gbrt-sklearn.html#sthash.Bwu6NdCJ.dpuf\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,n_estimators=40,max_depth=1,min_samples_split=500, min_samples_leaf=50, subsample=0.8, random_state=10),\n",
        "gsearch1 = RandomizedSearchCV(estimator = xgb.XGBRegressor(n_estimators = 1000,silent=True),\n",
        "  scoring='neg_mean_squared_error', \n",
        "  param_distributions  = XGparams, n_jobs=4,iid=False, cv=5)\n",
        "\n",
        "\n",
        "for randGeoHash in randGeoHashes:  \n",
        "  randDataSet=dataset[dataset.geohash6==randGeoHash]\n",
        "  #print(randGeoHash)\n",
        "  testData=randDataSet[-5:]\n",
        "  trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]  \n",
        "  trainData1,trainLabel1,testData1=splitData1(dataset[dataset.geohash6==randGeoHash], dataset) \n",
        "  \n",
        "  if(len(randDataSet) <= 5 or len(trainData) <= 1 or  len(trainData1) <= 5):\n",
        "    result = predictUnexpectedData(randGeoHash, trainData, testData)\n",
        "    for pred in trainedpreds:\n",
        "      trainedpreds[pred][randGeoHash]=result\n",
        "    continue  \n",
        "      \n",
        "  gsearch1.fit(trainData1,trainLabel1)\n",
        "\n",
        "gsearch1.best_params_, gsearch1.best_score_\n",
        "\n",
        "#\n",
        "#({'alpha': 0,\n",
        "#  'colsample_bytree': 0.3,\n",
        "#  'gamma': 0.5,\n",
        "#  'learning_rate': 0.1,\n",
        "#  'max_depth': 7,\n",
        "#  'min_child_weight': 10,\n",
        "#  'subsample': 1.0},\n",
        "3 -0.00021086111001349567)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no train data, just predict based on average of 0.105090695755598\n",
            "not enough train data, just predict based last train data\n",
            "not enough train data, just predict based last train data\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "not enough train data, just predict based last train data\n",
            "not enough train data, just predict based last train data\n",
            "not enough train data, just predict based last train data\n",
            "not enough train data, just predict based last train data\n",
            "not enough train data, just predict based last train data\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "no train data, just predict based on average of 0.105090695755598\n",
            "no train data, just predict based on average of 0.105090695755598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'alpha': 0,\n",
              "  'colsample_bytree': 0.3,\n",
              "  'gamma': 0.5,\n",
              "  'learning_rate': 0.1,\n",
              "  'max_depth': 7,\n",
              "  'min_child_weight': 10,\n",
              "  'subsample': 1.0},\n",
              " -0.00021086111001349567)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6cycBk59S7",
        "colab_type": "code",
        "outputId": "66c4019e-2f35-49cb-f6ed-25792af723d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# out of 300 random\n",
        "#0.02166  rsme for model  predictedXGBoost1 . Max RMSE is at index 215 with  0.12891 ,Min RMSE is at index 178 with  0.00109\n",
        "#0.01975  rsme for model  predictedXGBoost2 . Max RMSE is at index 215 with  0.12979 ,Min RMSE is at index 178 with  0.00109\n",
        "#0.01986  rsme for model  gradientBoostRegressor1 . Max RMSE is at index 85 with  0.10416 ,Min RMSE is at index 178 with  0.00109\n",
        "#0.01957  rsme for model  gradientBoostRegressor4 . Max RMSE is at index 85 with  0.10416 ,Min RMSE is at index 178 with  0.00109\n",
        "#0.02868  rsme for model  gradientBoostRegressor6 . Max RMSE is at index 215 with  0.23844 ,Min RMSE is at index 178 with  0.00109\n",
        "\n",
        "\n",
        "#hashToView='qp09kx'\n",
        "#aa=testDatas[hashToView]\n",
        "#aa['x1']=preds['predictedXGBoost1'][hashToView]\n",
        "#aa['x2']=preds['predictedXGBoost2'][hashToView]\n",
        "#aa['g1']=preds['gradientBoostRegressor1'][hashToView]\n",
        "#aa['g2']=preds['gradientBoostRegressor2'][hashToView]\n",
        "#aa\n",
        "\n",
        "#len(trainData6)-len(trainData5)\n",
        "\n",
        "#for pred in trainedTests:    \n",
        "trainedpreds={}\n",
        "trainedTests=['gbrt3',\n",
        "             ]\n",
        "             \n",
        "for pred in trainedTests:\n",
        "  trainedpreds[pred]={}\n",
        "\n",
        "for randGeoHash in randGeoHashes:  \n",
        "  trainData1,trainLabel1,testData1=splitData1(dataset[dataset.geohash6==randGeoHash], dataset)      \n",
        "  trainedpreds['gbrt3'][randGeoHash]=gbrt3.predict(testData1)\n",
        "  \n",
        "  #train4X,train4Y,test4X = create_RNNset(trainData4, trainLabel4, testData4) \n",
        "  #keras2.fit(train4X, train4Y, epochs=5, batch_size=1, verbose=0)      \n",
        "  #trainedpreds['reg8'][randGeoHash]=reg8.predict(testData1)\n",
        "  #trainedpreds['xg_reg8'][randGeoHash]=xg_reg8.predict(testData1)\n",
        "\n",
        "for pred in trainedTests:    \n",
        "  avgRMSE(trainedpreds[pred], testDatas, pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-855a7cceade1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainData4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplitData4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandDataSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#hashToView='qp09kx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#aa=testDatas[hashToView]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#aa['x1']=preds['predictedXGBoost1'][hashToView]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ea1a37b913ed>\u001b[0m in \u001b[0;36msplitData4\u001b[0;34m(hashDataset, allSet)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mtestData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashDataset1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0mtrainData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashDataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashDataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhashDataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m# remove first row since NA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2137\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttwmHYHvkD0b",
        "colab_type": "code",
        "outputId": "925c4c73-4ef1-42b1-c437-3d1a34ad04bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=30)\n",
        "X=trainData4.values\n",
        "#X=X.drop(['datetime','timestamp','geohash6'],axis=1).values\n",
        "testX=testData4.values\n",
        "#testX=testX.drop(['datetime','timestamp','geohash6'],axis=1).values\n",
        "\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(trainX)\n",
        "#plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
        "len(y_kmeans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 16, 16, 16, 16], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU4Tut8PkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Evh4Ug4ill8",
        "colab_type": "code",
        "outputId": "54c70566-0538-4db5-99d4-ecf41b94e532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset[dataset.day==61].demand.mean() / dataset[dataset.day==60].demand.mean() \n",
        "\n",
        "#trainData.groupby('dayOfWeek').mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8939873172901138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMLVGnG5TGlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#trainDataX\n",
        "\n",
        "#display(SVG(graph.pipe(format='svg')))\n",
        "#display.display(SVG(str_tree)\n",
        "#trainData1,trainLabel1,testData1=splitData1(randDataSet, dataset)\n",
        "#trainData2,trainLabel2,testData2=splitData2(randDataSet, dataset)\n",
        "#trainData3,trainLabel3,testData3=splitData3(randDataSet, dataset)\n",
        "#trainData4,trainLabel4,testData4=splitData4(randDataSet, dataset)\n",
        "\n",
        "#for row in randDataSet.itertuples():\n",
        "#randDataSet['neighborPrevDemand']=dataset.loc[randDataSet['geohash6']].mean()\n",
        "#print('length of 1st:',len(trainData1)+len(testData1),',2th:',len(trainData2)+len(testData2),',3th:',len(trainData3)+len(testData3),' 4th ',len(trainData4)+len(testData4))\n",
        "#trainData4\n",
        "#preds\n",
        "# TODO write method to find geohash with highest error and see why, how to improve\n",
        "\n",
        "#model_fit = arima_model.fit(trainData.demand, suppress_warnings=True)\n",
        "#trainData.demand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7A7bH2nfuX0",
        "colab_type": "code",
        "outputId": "adc7e7da-8fd1-4f59-f2a4-420ec52b9277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06192209, 0.05783723, 0.0600192 , 0.05783723, 0.06192209])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVnRC-g9_DMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictedRandForestRegressor1['qp03zx']\n",
        "randGeoHash='qp02yu'\n",
        "randDataSet=dataset.loc[dataset.geohash6==randGeoHash]\n",
        "\n",
        "testData=randDataSet[-5:]\n",
        "trainData=randDataSet.loc[(dataset.day>=testData.iloc[0].day-14) & (dataset.x<testData.iloc[0].x)]\n",
        "#trainData.plot(x='x',y='demand')\n",
        "#testData.plot(x='x',y='demand')\n",
        "\n",
        "#predictInvalidPreds('qp08fu',predictedArima2)\n",
        "#predictedArima2\n",
        "#np.isfinite(predictedArima1['qp03zx'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ8o9wI_-b-i",
        "colab_type": "text"
      },
      "source": [
        "#HeatMap with time to see if any ideas to improve model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bQlAXTfx3XQ-",
        "colab": {}
      },
      "source": [
        "#https://alysivji.github.io/getting-started-with-folium.html\n",
        "# max value seem like 999 even though manual say 0-1\n",
        "mapData=[]\n",
        "#time_in_x_slider = 7 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "for x in range(5850,dataset.x.max()): # max seem like 30\n",
        "  #range(0, dataset.x.max())\n",
        "  # skip so that faster but will missed some logic\n",
        "  #if(x % 500 != 0):\n",
        "  #   continue\n",
        "  subData=[]\n",
        "  for row in dataset.loc[dataset['x']==x].itertuples():\n",
        "    gh6decoded=geohash.decode(getattr(row,'geohash6'))      \n",
        "    subData.append([gh6decoded[0],gh6decoded[1],getattr(row,'demand')*10])\n",
        "  mapData.append(subData)\n",
        "        \n",
        "m=folium.Map([mapData[0][0][0],mapData[0][0][1]], zoom_start=10)\n",
        "m.add_child(plugins.HeatMapWithTime(mapData, radius=10, auto_play=True)) # seem like cannot change speed or loop\n",
        "m.add_child(folium.LatLngPopup())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odK7-7hv-fGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "72069bfe-f786-4c0b-b89e-b812c2e77c79"
      },
      "source": [
        "#https://stackoverflow.com/questions/3810865/matplotlib-unknown-projection-3d-error\n",
        "visData1=dataset[dataset.geohash6==np.random.choice(dataset.geohash6.unique(),2)[0]]\n",
        "trainDataa,trainLabela,testDataa=splitData10(visData1, dataset)\n",
        "visData2=dataset[dataset.geohash6==np.random.choice(dataset.geohash6.unique(),2)[0]]\n",
        "trainDatab,trainLabelb,testDatab=splitData10(visData2, dataset)\n",
        "trainDataa,trainLabela,testDataa=splitData10(dataset[dataset.geohash6=='qp0d4m'],dataset)\n",
        "\n",
        "#a=pd.DataFrame()\n",
        "#testDataa\n",
        "#trainDataa['demand']=trainLabel6a\n",
        "#trainDatab['demand']=trainLabel6b\n",
        "#a['6a']=trainDataa.groupby('timestampX').describe()\n",
        "#a['6b']=trainDatab.groupby('timestampX').describe()\n",
        "\n",
        "\n",
        "#a\n",
        "#randGeoHashes=\n",
        "#fig,ax1 = plt.subplots(figsize=(4,4))\n",
        "#ax1.scatter(trainData6a['timestampX'], trainLabel6a)\n",
        "#fig,ax2 = plt.subplots(figsize=(4,4))\n",
        "#ax2.scatter(trainData6b['timestampX'], trainLabel6b)\n",
        "#plt.show()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>timestampX</th>\n",
              "      <th>dayOfWeek</th>\n",
              "      <th>x</th>\n",
              "      <th>demandEmptyTimeCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3692545</th>\n",
              "      <td>42</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>3985</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         day  timestampX  dayOfWeek     x  demandEmptyTimeCount\n",
              "3692545   42          49          0  3985                   0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS6y_biDNEVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        },
        "outputId": "15f5a16a-2193-434e-ab6a-1870e1ac92ef"
      },
      "source": [
        "\n",
        "  "
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>timestampX</th>\n",
              "      <th>dayOfWeek</th>\n",
              "      <th>x</th>\n",
              "      <th>demand</th>\n",
              "      <th>demandEmptyTimeCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1052018</th>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4416</td>\n",
              "      <td>0.094914</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1947725</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4417</td>\n",
              "      <td>0.070080</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419870</th>\n",
              "      <td>47.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4418</td>\n",
              "      <td>0.097505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3860768</th>\n",
              "      <td>47.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4419</td>\n",
              "      <td>0.105218</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480279</th>\n",
              "      <td>47.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4420</td>\n",
              "      <td>0.135887</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3590441</th>\n",
              "      <td>47.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4421</td>\n",
              "      <td>0.088575</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966622</th>\n",
              "      <td>47.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4422</td>\n",
              "      <td>0.133793</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2269011</th>\n",
              "      <td>47.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4423</td>\n",
              "      <td>0.132499</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417416</th>\n",
              "      <td>47.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4424</td>\n",
              "      <td>0.156030</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403063</th>\n",
              "      <td>47.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4425</td>\n",
              "      <td>0.139918</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3411649</th>\n",
              "      <td>47.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4426</td>\n",
              "      <td>0.120847</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1787719</th>\n",
              "      <td>47.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4427</td>\n",
              "      <td>0.110508</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413402</th>\n",
              "      <td>47.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4428</td>\n",
              "      <td>0.150303</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1888738</th>\n",
              "      <td>47.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4429</td>\n",
              "      <td>0.116112</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246910</th>\n",
              "      <td>47.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4430</td>\n",
              "      <td>0.161992</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242588</th>\n",
              "      <td>47.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4431</td>\n",
              "      <td>0.157561</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796594</th>\n",
              "      <td>47.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4432</td>\n",
              "      <td>0.161384</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2824895</th>\n",
              "      <td>47.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4433</td>\n",
              "      <td>0.117888</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243995</th>\n",
              "      <td>47.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4434</td>\n",
              "      <td>0.187400</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414058</th>\n",
              "      <td>47.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4435</td>\n",
              "      <td>0.228560</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2714148</th>\n",
              "      <td>47.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4436</td>\n",
              "      <td>0.166844</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638386</th>\n",
              "      <td>47.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4437</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3741523</th>\n",
              "      <td>47.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4438</td>\n",
              "      <td>0.128807</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189826</th>\n",
              "      <td>47.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4439</td>\n",
              "      <td>0.106432</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059971</th>\n",
              "      <td>47.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4440</td>\n",
              "      <td>0.175479</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538860</th>\n",
              "      <td>47.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4441</td>\n",
              "      <td>0.170013</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3044423</th>\n",
              "      <td>47.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4442</td>\n",
              "      <td>0.190576</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2918889</th>\n",
              "      <td>47.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4443</td>\n",
              "      <td>0.188502</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3248875</th>\n",
              "      <td>47.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4444</td>\n",
              "      <td>0.148545</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048305</th>\n",
              "      <td>47.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4445</td>\n",
              "      <td>0.209321</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571043</th>\n",
              "      <td>61.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5821</td>\n",
              "      <td>0.015473</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2662014</th>\n",
              "      <td>61.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5822</td>\n",
              "      <td>0.031754</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621199</th>\n",
              "      <td>61.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5823</td>\n",
              "      <td>0.021353</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3889916</th>\n",
              "      <td>61.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5824</td>\n",
              "      <td>0.033428</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2861243</th>\n",
              "      <td>61.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5825</td>\n",
              "      <td>0.016462</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3428729</th>\n",
              "      <td>61.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5826</td>\n",
              "      <td>0.023433</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577897</th>\n",
              "      <td>61.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5827</td>\n",
              "      <td>0.014455</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>61.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89233</th>\n",
              "      <td>61.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5829</td>\n",
              "      <td>0.014425</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>61.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>61.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5831</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>61.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3945991</th>\n",
              "      <td>61.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5833</td>\n",
              "      <td>0.001962</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>61.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5834</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>61.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>61.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>61.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>61.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>61.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5839</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3181119</th>\n",
              "      <td>61.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5840</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>61.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>61.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5842</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>61.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>61.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>61.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>61.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3861950</th>\n",
              "      <td>61.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5847</td>\n",
              "      <td>0.024385</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994036</th>\n",
              "      <td>61.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5848</td>\n",
              "      <td>0.006909</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4008519</th>\n",
              "      <td>61.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5849</td>\n",
              "      <td>0.026236</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3909331</th>\n",
              "      <td>61.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5850</td>\n",
              "      <td>0.016301</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1435 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          day  timestampX  dayOfWeek     x    demand  demandEmptyTimeCount\n",
              "1052018  47.0         0.0        5.0  4416  0.094914                   0.0\n",
              "1947725  47.0         1.0        5.0  4417  0.070080                   1.0\n",
              "419870   47.0         2.0        5.0  4418  0.097505                   0.0\n",
              "3860768  47.0         3.0        5.0  4419  0.105218                   0.0\n",
              "2480279  47.0         4.0        5.0  4420  0.135887                   0.0\n",
              "3590441  47.0         5.0        5.0  4421  0.088575                   0.0\n",
              "966622   47.0         6.0        5.0  4422  0.133793                   0.0\n",
              "2269011  47.0         7.0        5.0  4423  0.132499                   0.0\n",
              "2417416  47.0         8.0        5.0  4424  0.156030                   0.0\n",
              "1403063  47.0         9.0        5.0  4425  0.139918                   0.0\n",
              "3411649  47.0        10.0        5.0  4426  0.120847                   0.0\n",
              "1787719  47.0        11.0        5.0  4427  0.110508                   0.0\n",
              "1413402  47.0        12.0        5.0  4428  0.150303                   0.0\n",
              "1888738  47.0        13.0        5.0  4429  0.116112                   0.0\n",
              "246910   47.0        14.0        5.0  4430  0.161992                   0.0\n",
              "1242588  47.0        15.0        5.0  4431  0.157561                   0.0\n",
              "1796594  47.0        16.0        5.0  4432  0.161384                   0.0\n",
              "2824895  47.0        17.0        5.0  4433  0.117888                   0.0\n",
              "243995   47.0        18.0        5.0  4434  0.187400                   0.0\n",
              "1414058  47.0        19.0        5.0  4435  0.228560                   0.0\n",
              "2714148  47.0        20.0        5.0  4436  0.166844                   0.0\n",
              "638386   47.0        21.0        5.0  4437  0.132349                   0.0\n",
              "3741523  47.0        22.0        5.0  4438  0.128807                   0.0\n",
              "2189826  47.0        23.0        5.0  4439  0.106432                   0.0\n",
              "3059971  47.0        24.0        5.0  4440  0.175479                   0.0\n",
              "1538860  47.0        25.0        5.0  4441  0.170013                   0.0\n",
              "3044423  47.0        26.0        5.0  4442  0.190576                   0.0\n",
              "2918889  47.0        27.0        5.0  4443  0.188502                   0.0\n",
              "3248875  47.0        28.0        5.0  4444  0.148545                   0.0\n",
              "1048305  47.0        29.0        5.0  4445  0.209321                   0.0\n",
              "...       ...         ...        ...   ...       ...                   ...\n",
              "571043   61.0        61.0        5.0  5821  0.015473                   0.0\n",
              "2662014  61.0        62.0        5.0  5822  0.031754                   1.0\n",
              "1621199  61.0        63.0        5.0  5823  0.021353                   2.0\n",
              "3889916  61.0        64.0        5.0  5824  0.033428                   4.0\n",
              "2861243  61.0        65.0        5.0  5825  0.016462                   5.0\n",
              "3428729  61.0        66.0        5.0  5826  0.023433                   6.0\n",
              "577897   61.0        67.0        5.0  5827  0.014455                   9.0\n",
              "260      61.0        68.0        5.0  5828  0.000000                  12.0\n",
              "89233    61.0        69.0        5.0  5829  0.014425                   7.0\n",
              "261      61.0        70.0        5.0  5830  0.000000                  10.0\n",
              "262      61.0        71.0        5.0  5831  0.000000                  13.0\n",
              "263      61.0        72.0        5.0  5832  0.000000                  10.0\n",
              "3945991  61.0        73.0        5.0  5833  0.001962                  14.0\n",
              "264      61.0        74.0        5.0  5834  0.000000                  15.0\n",
              "265      61.0        75.0        5.0  5835  0.000000                  10.0\n",
              "266      61.0        76.0        5.0  5836  0.000000                  10.0\n",
              "267      61.0        77.0        5.0  5837  0.000000                  15.0\n",
              "268      61.0        78.0        5.0  5838  0.000000                  15.0\n",
              "269      61.0        79.0        5.0  5839  0.000000                  14.0\n",
              "3181119  61.0        80.0        5.0  5840  0.006409                  14.0\n",
              "270      61.0        81.0        5.0  5841  0.000000                  15.0\n",
              "271      61.0        82.0        5.0  5842  0.000000                  13.0\n",
              "272      61.0        83.0        5.0  5843  0.000000                  13.0\n",
              "273      61.0        84.0        5.0  5844  0.000000                  14.0\n",
              "274      61.0        85.0        5.0  5845  0.000000                  14.0\n",
              "275      61.0        86.0        5.0  5846  0.000000                  10.0\n",
              "3861950  61.0        87.0        5.0  5847  0.024385                   5.0\n",
              "994036   61.0        88.0        5.0  5848  0.006909                   3.0\n",
              "4008519  61.0        89.0        5.0  5849  0.026236                   2.0\n",
              "3909331  61.0        90.0        5.0  5850  0.016301                   0.0\n",
              "\n",
              "[1435 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-CMXPsobJm",
        "colab_type": "text"
      },
      "source": [
        "# Model Performance\n",
        "### \"Your model can use features of up to 14 consecutive days from the test dataset, ending at timestamp T and predict T+1 to T+5.\"\n",
        "### \"Geohash coverage: You may assume that the set of geohashes are the same in training dataset and test dataset. The original geohashes are anonymised, but you may assume that adjacency is maintained between the geohashes.\"\"\n",
        "### \"Submissions will be evaluated by RMSE (root mean squared error) averaged over all geohash6, 15-minute-bucket pairs.\""
      ]
    }
  ]
}